

--- Tools in Data Science ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Tools in Data Science - May 2025

Tools in Data Science is a practical diploma level data science course at IIT Madras that teaches popular tools for sourcing data, transforming it, analyzing it, communicating these as visual stories, and deploying them in production.

This course exposes you to real-life tools
This course is quite hard
But it's probably worth it.
Programming skills are a pre-requisite
If you passed, don't enroll again
We encourage learning by sharing

You CAN copy from friends. You can work in groups. You can share code. Even in projects, assignments, and exams (except the final end-term exam).

Why should you copy? Because in real life, there’s no time to re-invent the wheel. You’ll be working in teams on the shoulders of giants. It’s important to learn how to do that well.

To learn well, understand what you’re copying. If you’re short of time, prioritize.

To learn better, teach what you’ve learnt.

We cover 7 modules in 12 weeks

The content evolves with technology and feedback. Track the commit history for changes.

Development Tools and concepts to build models and apps.
Deployment Tools and concepts to publish what you built.
Large Language Models that make your work easier and your apps smarter.
Data Sourcing to get data from the web, files, and databases.
Data Preparation to clean up and convert the inputs to the right format.
Data Analysis to find surprising insights in the data.
Data Visualization to communicate those insights as visual stories.
Anyone can audit this course

Everyone has access to:

Course content at https://tds.s-anand.net/
Evaluations

You can solve these questions any time and check your answers before the submission dates.

Only enrolled students can participate in Discourse, get project evaluations, take the final end-term, or get a certificate.

Those auditing can join the TDS 2025 May Google Group for announcements.

Evaluations are mostly open Internet

Tentative dates:

Exam	Type	Weight	Release Date	Submission Date
GA: Graded assignments	Best 4 out of 7	15%		
Graded Assignment 1 Discuss	Online open-Internet MCQ		Thu 01 May 2025	Sun 18 May 2025
Graded Assignment 2 Discuss	Online open-Internet MCQ		Thu 05 May 2025	Sun 25 May 2025
Graded Assignment 3 Discuss	Online open-Internet MCQ		Fri 20 May 2025	Sun 01 Jun 2025
P1: Project 1 Discuss	Online open-Internet	20%	Fri 16 May 2025	Sat 14 Jun 2025
Graded Assignment 4 Discuss	Online open-Internet MCQ		Wed 14 Jun 2025	Sun 22 Jun 2025
Graded Assignment 5	Online open-Internet MCQ		Wed 18 Jun 2025	Sun 29 Jun 2025
Graded Assignment 6	Online open-Internet MCQ		Wed 02 Jul 2025	Sun 13 Jul 2025
ROE: Remote Online Exam	Online open-Internet MCQ	20%	Sun 20 Jul 2025 13:00	Sun 20 Jul 2025 13:45
Graded Assignment 7	Online open-Internet MCQ		Wed 16 Jul 2025	Sun 27 Jul 2025
P2: Project 2	Online open-Internet	20%	Fri 11 Jul 2025	Fri 8 Aug 2025
F: Final end-term	In-person, no internet	25%	Sun 31 Aug 2025	Sun 31 Aug 2025

Updates:

5 May 2025:
GA1 submission date postponed from 11 May to 18 May 2025
GA2 submission date postponed from 18 May to 25 May 2025
GA3 submission date preponed from 01 Jun to 25 May 2025 since there’s a break the week after
P1 submission date postponed from 14 Jun to 18 Jun 2025
GA4 release date delayed to 14 Jun 2025
Notes
Graded Assignment 1 checks course pre-requisites. Please drop this course (do it in a later term) if you score low. It’ll be too tough for you now.
Remote exams are open and hard
You can use the Internet, WhatsApp, ChatGPT, your notes, your friends, your pets…
The RoE is especially hard. Read: What is the purpose of an impossible RoE exam?
Final exam is in-person and closed book. It tests your memory. It’s easy.
Projects test application. The projects test how well you apply what you learnt in a real-world context.
Bonus activities may be posted on Discourse. See previous bonus activities
Evaluations are mostly automated. This course uses pre-computed (for objective) or LLMs (for subjective) evaluations.
LLMs will evaluate you differently each time. Learn to prompt them robustly to get higher marks.
Constantly check communications

Check these three links regularly to keep up with the course.

Seek Notifications for Course Notifications. Log into seek.onlinedegree.iitm.ac.in and click on the bell icon  on the top right corner . Check notifications daily. 
Your email for Course Announcements. Seek Inbox are forwarded to your email. Check daily. Check spam folders too.
TDS Discourse: Faculty, instructors, and TAs will share updates and address queries here. Email support@study.iitm.ac.in cc: discourse-staff1@study.iitm.ac.in if you can’t access Discourse.
People who help you
Faculty (who design the course)
Anand S, root.node@gmail.com | @s.anand
Instructors (who teach the course)
Carlton D’Silva. 22f3001919@ds.study.iitm.ac.in | @carlton
Prasanna S, prasanna@study.iitm.ac.in | @iamprasna
Teaching assistants (who help you with your doubts)
Jivraj Singh, 22f3002542@ds.study.iitm.ac.in | @Jivraj | LinkedIn Profile
Suchintika Sarkar, 21f3002441@ds.study.iitm.ac.in | @21f3002441
Hritik Roshan Maurya, 22f3002460@ds.study.iitm.ac.in | @HritikRoshan_HRM
Saransh Saini, 22f1001123@ds.study.iitm.ac.in | @Saransh_Saini | LinkedIn Profile
Virtual TA (GPT Instructions)

Their job is to help you. Trouble them for your slightest doubts!

Course Links
TDS: Discourse - Ask questions, get help, and discuss with your peers.
IITM BS Degree Programme - Student Handbook
TDS: Public course page
TDS: Course files – Jupyter notebooks, datasets, etc.
May 2025 Links
Grading Document - May 2025
TDS: Course page - May 2025 – for students to access course content.
TDS: Announcement group - May 2025
TDS: Live Sessions
Past Course Content
TDS: Course Content - Jan 2025
TDS: Live Sessions - Jan 2025 – YouTube playlist
TDS: Course calendar - Jan 2025
Grading Document - Jan 2025.
Next 
1. Development Tools


--- 1. Development Tools ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Development Tools

NOTE: The tools in this module are PRE-REQUISITES for the course. You would have used most of these before. If most of this is new to you, please take this course later.

Some tools are fundamental to data science because they are industry standards and widely used by data science professionals. Mastering these tools will align you with current best practices and making you more adaptable in a fast-evolving industry.

The tools we cover here are not just popular, they’re the core technology behind most of today’s data science and software development.

 Previous
Tools in Data Science
Next 
Editor: VS Code


--- Editor: VS Code ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Editor: VS Code

Your editor is the most important tool in your arsenal. That’s where you’ll spend most of your time. Make sure you’re comfortable with it.

Visual Studio Code is, by far, the most popular code editor today. According to the 2024 StackOverflow Survey almost 75% of developers use it. We recommend you learn it well. Even if you use another editor, you’ll be working with others who use it, and it’s a good idea to have some exposure.

Watch these introductory videos (35 min) from the Visual Studio Docs to get started:

Getting Started: Set up and learn the basics of Visual Studio Code. (7 min)
Code Editing: Learn how to edit and run code in VS Code. (3 min)
Productivity Tips: Become a VS Code power user with these productivity tips. (4 min)
Personalize: Personalize VS Code to make it yours with themes. (2 min)
Extensions: Add features, themes, and more to VS Code with extensions! (4 min)
Debugging: Get started with debugging in VS Code. (6 min)
Version Control: Learn how to use Git version control in VS Code. (3 min)
Customize: Learn how to customize your settings and keyboard shortcuts in VS Code. (6 min)
 Previous
1. Development Tools
Next 
AI Code Editors: GitHub Copilot


--- AI Code Editors: GitHub Copilot ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
AI Editor: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
AI Editor: GitHub Copilot

AI Code Editors like GitHub Copilot, Cursor, Windsurf, Roo Code, Cline, Continue.dev, etc. use LLMs to help you write code faster.

Most are built on top of VS Code. These are now a standard tool in every developer’s toolkit.

GitHub Copilot is free (as of May 2025) for 2,000 completions and 50 chats.

You should learn about:

Code Suggestions, which is a basic feature.
Using Chat, which lets you code in natural language.
Changing the chat model. The free version includes Claude 3.5 Sonnet, a good coding model.
Prompts to understand how people use AI code editors.
 Previous
Editor: VS Code
Next 
Python tools: uv


--- Python tools: uv ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Python tools: uv

Install uv.

uv is a fast Python package and project manager that’s becoming the standard for running Python scripts. It replaces tools like pip, conda, pipx, poetry, pyenv, twine, and virtualenv into one, enabling:

Python Version Management: uv installs and manages multiple Python versions, allowing developers to specify and switch between versions seamlessly.
Virtual Environment Handling: It automates the creation and management of virtual environments, ensuring isolated and consistent development spaces for different projects.
Dependency Management: With support for the pyproject.toml format, uv enables precise specification of project dependencies. It maintains a universal lockfile, uv.lock, to ensure reproducible installations across different systems.
Project Execution: The uv run command allows for the execution of scripts and applications within the managed environment, streamlining development workflows.

Here are some commonly used commands:

# Replace python with uv. This automatically installs Python and dependencies.
uv run script.py

# Run a Python script directly from the Internet
uv run https://example.com/script.py

# Run a Python script without installing
uvx ruff

# Use a specific Python version
uv run --python 3.11 script.py

# Add dependencies to your script
uv add httpx --script script.py

# Create a virtual environment at .venv
uv venv

# Install packages to your virtual environment
uv pip install httpx
Copy to clipboard
Error
Copied

Here are some useful tools you can run with uvx without installation:

uvx --from jupyterlab jupyter-lab   # Jupyter notebook
uvx marimo      # Interactive notebook
uvx llm         # Chat with LLMs from the command line
uvx openwebui   # Chat with LLMs via the browser
uvx httpie      # Make HTTP requests
uvx datasette   # Browse SQLite databases
uvx markitdown  # Convert PDF to Markdown
uvx yt-dlp      # Download YouTube videos
uvx asciinema   # Record your terminal and play it
Copy to clipboard
Error
Copied

uv uses inline script metadata for dependencies. The eliminates the need for requirements.txt or virtual environments. For example:

# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
#   "pandas",
# ]
# ///
Copy to clipboard
Error
Copied

 Previous
AI Code Editors: GitHub Copilot
Next 
JavaScript tools: npx


--- JavaScript tools: npx ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
JavaScript tools: npx

npx is a command-line tool that comes with npm (Node Package Manager) and allows you to execute npm package binaries and run one-off commands without installing them globally. It’s essential for modern JavaScript development and data science workflows.

For data scientists, npx is useful when:

Running JavaScript-based data visualization tools
Converting notebooks and documents
Testing and formatting code
Running development servers

Here are common npx commands:

# Run a package without installing
npx http-server .                # Start a local web server
npx prettier --write .           # Format code or docs
npx eslint .                     # Lint JavaScript
npx typescript-node script.ts    # Run TypeScript directly
npx esbuild app.js               # Bundle JavaScript
npx jsdoc .                      # Generate JavaScript docs

# Run specific versions
npx prettier@3.2 --write .        # Use prettier 3.2

# Execute remote scripts (use with caution!)
npx github:user/repo            # Run from GitHub
Copy to clipboard
Error
Copied

Watch this introduction to npx (6 min):

 Previous
Python tools: uv
Next 
Unicode


--- Unicode ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Unicode

Ever noticed when you copy-paste some text and get garbage symbols? Or see garbage when you load a CSV file? This video explains why. It covers how computers store text (called character encoding) and why it sometimes goes wonky.

Learn about ASCII (the original 7-bit encoding system that could only handle 128 characters), why that wasn’t enough for global languages, and how modern solutions like Unicode save the day by letting us use any character from any language.

Some programs try to guess encodings (sometimes badly!). A signature called BOM (Byte Order Mark)helps computers know exactly how to read text files correctly.

Learn how Unicode, UTF-8 and character encoding works. This is a common gotcha when building apps that handle international text - something bootcamps often skip but developers and data scientists regularly face in the real world.

Unicode is fundamental for data scientists working with international data. Here are key concepts you need to understand:

Character Encodings: Different ways to represent text in computers
ASCII (7-bit): Limited to 128 characters, English-only
UTF-8: Variable-width encoding, backwards compatible with ASCII
UTF-16: Fixed-width encoding, used in Windows and Java
UTF-32: Fixed-width encoding, memory inefficient but simple

Common encoding issues you’ll encounter:

# Reading files with explicit encoding
with open('file.txt', encoding='utf-8') as f:
    text = f.read()

# Handling encoding errors
import pandas as pd
df = pd.read_csv('data.csv', encoding='utf-8', errors='replace')

# Detecting file encoding
import chardet
with open('unknown.txt', 'rb') as f:
    result = chardet.detect(f.read())
print(result['encoding'])
Copy to clipboard
Error
Copied

 Previous
JavaScript tools: npx
Next 
Browser: DevTools


--- Browser: DevTools ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Browser: DevTools

Chrome DevTools is the de facto standard for web development and data analysis in the browser. You’ll use this a lot when debugging and inspecting web pages.

Here are the key features you’ll use most:

Elements Panel

Inspect and modify HTML/CSS in real-time
Copy CSS selectors for web scraping
Debug layout issues with the Box Model
// Copy selector in Console
copy($0); // Copies selector of selected element
Copy to clipboard
Error
Copied

Console Panel

JavaScript REPL environment
Log and debug data
Common console methods:
console.table(data); // Display data in table format
console.group("Name"); // Group related logs
console.time("Label"); // Measure execution time
Copy to clipboard
Error
Copied

Network Panel

Monitor API requests and responses
Simulate slow connections
Right-click on a request and select “Copy as fetch” to get the request.

Essential Keyboard Shortcuts

Ctrl+Shift+I (Windows) / Cmd+Opt+I (Mac): Open DevTools
Ctrl+Shift+C: Select element to inspect
Ctrl+L: Clear console
$0: Reference currently selected element
$$('selector'): Query selector all (returns array)

Videos from Chrome Developers (37 min total):

Fun & powerful: Intro to Chrome DevTools (5 min)
Different ways to open Chrome DevTools (5 min)
Faster DevTools navigation with shortcuts and settings (3 min)
How to log messages in the Console (6 min)
How to speed up your workflow with Console shortcuts (6 min)
HTML vs DOM? Let’s debug them (5 min)
Caching demystified: Inspect, clear, and disable caches (7 min)
Console message logging (6 min)
Console workflow shortcuts (6 min)
HTML vs DOM debugging (5 min)
Cache inspection and management (7 min)
 Previous
Unicode
Next 
CSS Selectors


--- CSS Selectors ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
CSS Selectors

CSS selectors are patterns used to select and style HTML elements on a web page. They are fundamental to web development and data scraping, allowing you to precisely target elements for styling or extraction.

For data scientists, understanding CSS selectors is crucial when:

Web scraping with tools like Beautiful Soup or Scrapy
Selecting elements for browser automation with Selenium
Styling data visualizations and web applications
Debugging website issues using browser DevTools

Watch this comprehensive introduction to CSS selectors (20 min):

The Mozilla Developer Network (MDN) provides detailed documentation on the three main types of selectors:

Basic CSS selectors: Learn about element (div), class (.container), ID (#header), and universal (*) selectors
Attribute selectors: Target elements based on their attributes or attribute values ([type="text"])
Combinators: Use relationships between elements (div > p, div + p, div ~ p)

Practice your CSS selector skills with this interactive tool:

CSS Diner: A fun game that teaches CSS selectors through increasingly challenging levels
 Previous
Browser: DevTools
Next 
JSON


--- JSON ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
JSON

JSON (JavaScript Object Notation) is the de facto standard format for data exchange on the web and APIs. Its human-readable format and widespread support make it essential for data scientists working with web services, APIs, and configuration files.

For data scientists, JSON is essential when:

Working with REST APIs and web services
Storing configuration files and metadata
Parsing semi-structured data from databases like MongoDB
Creating data visualization specifications (e.g., Vega-Lite)

Watch this comprehensive introduction to JSON (15 min):

Key concepts to understand in JSON:

JSON only supports 6 data types: strings, numbers, booleans, null, arrays, and objects
You can nest data. Arrays and objects can contain other data types, including other arrays and objects
Always validate. Ensure JSON is well-formed. Comm errors: Trailing commas, missing quotes, and escape characters

JSON Lines is a format that allows you to store multiple JSON objects in a single line. It’s useful for logging and streaming data.

Tools you could use with JSON:

JSONLint: Validate and format JSON
JSON Editor Online: Visual JSON editor and formatter
JSON Schema: Define the structure of your JSON data
jq: Command-line JSON processor

Common Python operations with JSON:

import json

# Parse JSON string
json_str = '{"name": "Alice", "age": 30}'
data = json.loads(json_str)

# Convert to JSON string
json_str = json.dumps(data, indent=2)

# Read JSON from file
with open('data.json') as f:
    data = json.load(f)

# Write JSON to file
with open('output.json', 'w') as f:
    json.dump(data, f, indent=2)

# Read JSON data a Pandas DataFrame. JSON data is typically stored as an array of objects.
import pandas as pd
df = pd.read_json('data.json')

# Read JSON lines from file into a DataFrame. JSON lines are typically one line per object.
df = pd.read_json('data.jsonl', lines=True)
Copy to clipboard
Error
Copied

Practice JSON skills with these resources:

JSON Generator: Create sample JSON data
JSON Path Finder: Learn to navigate complex JSON structures
JSON Schema Validator: Validate JSON against schemas
 Previous
CSS Selectors
Next 
Terminal: Bash


--- Terminal: Bash ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Terminal: Bash

UNIX shells are the de facto standard in the data science world and Bash is the most popular. This is available by default on Mac and Linux.

On Windows, install Git Bash or WSL to get a UNIX shell.

Watch this video to install WSL (12 min).

Watch this video to understand the basics of Bash and UNIX shell commands (75 min).

Essential Commands:

# File Operations
ls -la               # List all files with details
cd path/to/dir       # Change directory
pwd                  # Print working directory
cp source dest       # Copy files
mv source dest       # Move/rename files
rm -rf dir           # Remove directory recursively

# Text Processing
grep "pattern" file  # Search for pattern
sed 's/old/new/' f   # Replace text
awk '{print $1}' f   # Process text by columns
cat file | wc -l     # Count lines

# Process Management
ps aux               # List processes
kill -9 PID          # Force kill process
top                  # Monitor processes
htop                 # Interactive process viewer

# Network
curl url             # HTTP requests
wget url             # Download files
nc -zv host port     # Test connectivity
ssh user@host        # Remote login

# Count unique values in CSV column
cut -d',' -f1 data.csv | sort | uniq -c

# Quick data analysis
awk -F',' '{sum+=$2} END {print sum/NR}' data.csv  # Average
sort -t',' -k2 -n data.csv | head                  # Top 10

# Monitor log in real-time
tail -f log.txt | grep --color 'ERROR'
Copy to clipboard
Error
Copied

Bash Scripting Essentials:

#!/bin/bash

# Variables
NAME="value"
echo $NAME

# Loops
for i in {1..5}; do
    echo $i
done

# Conditionals
if [ -f "file.txt" ]; then
    echo "File exists"
fi

# Functions
process_data() {
    local input=$1
    echo "Processing $input"
}
Copy to clipboard
Error
Copied

Productivity Tips:

Command History

history         # Show command history
Ctrl+R         # Search history
!!             # Repeat last command
!$             # Last argument
Copy to clipboard
Error
Copied

Directory Navigation

pushd dir      # Push directory to stack
popd           # Pop directory from stack
cd -           # Go to previous directory
Copy to clipboard
Error
Copied

Job Control

command &      # Run in background
Ctrl+Z         # Suspend process
bg             # Resume in background
fg             # Resume in foreground
Copy to clipboard
Error
Copied

Useful Aliases - typically added to ~/.bashrc

alias ll='ls -la'
alias gs='git status'
alias jupyter='jupyter notebook'
alias activate='source venv/bin/activate'
Copy to clipboard
Error
Copied
 Previous
JSON
Next 
AI Terminal Tools: llm


--- AI Terminal Tools: llm ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
LLM CLI: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
LLM CLI: llm

llm is a command-line utility for interacting with large language models—simplifying prompts, managing models and plugins, logging every conversation, and extracting structured data for pipelines.

Basic Usage

Install llm. Then set up your OPENAI_API_KEY environment variable. See Getting started.

TDS Students: See Large Language Models for instructions on how to get and use OPENAI_API_KEY.

# Run a simple prompt
llm 'five great names for a pet pelican'

# Continue a conversation
llm -c 'now do walruses'

# Start a memory-aware chat session
llm chat

# Specify a model
llm -m gpt-4.1-nano 'Summarize tomorrow’s meeting agenda'

# Extract JSON output
llm 'List the top 5 Python viz libraries with descriptions' \
  --schema-multi 'name,description'
Copy to clipboard
Error
Copied

Or use llm without installation using uvx:

# Run llm via uvx without any prior installation
uvx llm 'Translate "Hello, world" into Japanese'

# Specify a model
uvx llm -m gpt-4.1-nano 'Draft a 200-word blog post on data ethics'

# Use structured JSON output
uvx llm 'List the top 5 programming languages in 2025 with their release years' \
  --schema-multi 'rank,language,release_year'
Copy to clipboard
Error
Copied
Key Features
Interactive prompts: llm '…' — Fast shell access to any LLM.
Conversational flow: -c '…' — Continue context across prompts.
Model switching: -m MODEL — Use OpenAI, Anthropic, local models, and more.
Structured output: llm json — Produce JSON for automation.
Logging & history: llm logs path — Persist every prompt/response in SQLite.
Web UI: datasette "$(llm logs path)" — Browse your entire history with Datasette.
Persistent chat: llm chat — Keep the model in memory across multiple interactions.
Plugin ecosystem: llm install PLUGIN — Add support for new models, data sources, or workflows. (Language models on the command-line - Simon Willison’s Weblog)
Practical Uses
Automated coding. Generate code scaffolding, review helpers, or utilities on demand. For example, after runningllm install llm-cmd, run llm cmd 'Undo the last git commit'. Inspired by Simon’s post on using LLMs for rapid tool building.
Transcript processing. Summarize YouTube or podcast transcripts using Gemini. See Putting Gemini 2.5 Pro through its paces.
Commit messages. Turn diffs into descriptive commit messages, e.g. git diff | llm 'Write a concise git commit message explaining these changes'. \
Data extraction. Convert free-text into structured JSON for automation. Structured data extraction from unstructured content using LLM schemas.
 Previous
Terminal: Bash
Next 
Spreadsheet: Excel, Google Sheets


--- Spreadsheet: Excel, Google Sheets ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Spreadsheet: Excel, Google Sheets

You’ll use spreadsheets for data cleaning and exploration. The most popular spreadsheet program is Microsoft Excel followed by Google Sheets.

You may be already familiar with these. If not, make sure to learn the basics of both.

Go through the Microsoft Excel video training and make sure you cover:

Intro to Excel
Rows & columns
Cells
Formatting
Formulas & Functions
Tables
PivotTables

Watch this video for an introduction to Google Sheets (49 min):

 Previous
AI Terminal Tools: llm
Next 
Database: SQLite


--- Database: SQLite ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Database: SQLite
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Database: SQLite

Relational databases are used to store data in a structured way. You’ll often access databases created by others for analysis.

PostgreSQL, MySQL, MS SQL, Oracle, etc. are popular databases. But the most installed database is SQLite. It’s embedded into many devices and apps (e.g. your phone, browser, etc.). It’s lightweight but very scalable and powerful.

Watch these introductory videos to understand SQLite and how it’s used in Python (34 min):

There are many non-relational databases (NoSQL) like ElasticSearch, MongoDB, Redis, etc. that you should know about and we may cover later.

Core Concepts:

-- Create a table
CREATE TABLE users (
    id INTEGER PRIMARY KEY,
    name TEXT NOT NULL,
    email TEXT UNIQUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Insert data
INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com');

-- Query data
SELECT name, COUNT(*) as count
FROM users
GROUP BY name
HAVING count > 1;

-- Join tables
SELECT u.name, o.product
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE o.status = 'pending';
Copy to clipboard
Error
Copied

Python Integration:

import sqlite3
from pathlib import Path
import pandas as pd

async def query_database(db_path: Path, query: str) -> pd.DataFrame:
    """Execute SQL query and return results as DataFrame.

    Args:
        db_path: Path to SQLite database
        query: SQL query to execute

    Returns:
        DataFrame with query results
    """
    try:
        conn = sqlite3.connect(db_path)
        return pd.read_sql_query(query, conn)
    finally:
        conn.close()

# Example usage
db = Path('data.db')
df = await query_database(db, '''
    SELECT date, COUNT(*) as count
    FROM events
    GROUP BY date
''')
Copy to clipboard
Error
Copied

Common Operations:

Database Management

-- Backup database
.backup 'backup.db'

-- Import CSV
.mode csv
.import data.csv table_name

-- Export results
.headers on
.mode csv
.output results.csv
SELECT * FROM table;
Copy to clipboard
Error
Copied

Performance Optimization

-- Create index
CREATE INDEX idx_user_email ON users(email);

-- Analyze query
EXPLAIN QUERY PLAN
SELECT * FROM users WHERE email LIKE '%@example.com';

-- Show indexes
SELECT * FROM sqlite_master WHERE type='index';
Copy to clipboard
Error
Copied

Data Analysis

-- Time series aggregation
SELECT
    date(timestamp),
    COUNT(*) as events,
    AVG(duration) as avg_duration
FROM events
GROUP BY date(timestamp);

-- Window functions
SELECT *,
    AVG(amount) OVER (
        PARTITION BY user_id
        ORDER BY date
        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW
    ) as moving_avg
FROM transactions;
Copy to clipboard
Error
Copied

Tools to work with SQLite:

SQLiteStudio: Lightweight GUI
DBeaver: Full-featured GUI
sqlite-utils: CLI tool
Datasette: Web interface
 Previous
Spreadsheet: Excel, Google Sheets
Next 
Version Control: Git, GitHub


--- Version Control: Git, GitHub ---

Tools in Data Science
Tools in Data Science
1. Development Tools
Editor: VS Code
AI Code Editors: GitHub Copilot
Python tools: uv
JavaScript tools: npx
Unicode
Browser: DevTools
CSS Selectors
JSON
Terminal: Bash
AI Terminal Tools: llm
Spreadsheet: Excel, Google Sheets
Database: SQLite
Version Control: Git, GitHub
Version Control: Git, GitHub
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Version Control: Git, GitHub

Git is the de facto standard for version control of software (and sometimes, data as well). It’s a system that keeps track of changes you make to files and folders. It allows you to revert to a previous state, compare changes, etc. It’s a central tool in any developer’s workflow.

GitHub is the most popular hosting service for Git repositories. It’s a website that shows your code, allows you to collaborate with others, and provides many useful tools for developers.

Watch these introductory videos to learn the basics of Git and GitHub (98 min):

Essential Git Commands:

# Repository Setup
git init                   # Create new repo
git clone url              # Clone existing repo
git remote add origin url  # Connect to remote

# Basic Workflow
git status                 # Check status
git add .                  # Stage all changes
git commit -m "message"    # Commit changes
git push origin main       # Push to remote

# Branching
git branch                 # List branches
git checkout -b feature    # Create/switch branch
git merge feature          # Merge branch
git rebase main            # Rebase on main

# History
git log --oneline          # View history
git diff commit1 commit2   # Compare commits
git blame file             # Show who changed what
Copy to clipboard
Error
Copied

Best Practices:

Commit Messages

# Good commit message format
type(scope): summary

Detailed description of changes.

# Examples
feat(api): add user authentication
fix(db): handle null values in query
Copy to clipboard
Error
Copied

Branching Strategy

main: Production code
develop: Integration branch
feature/*: New features
hotfix/*: Emergency fixes

Code Review

Keep PRs small (<400 lines)
Use draft PRs for WIP
Review your own code first
Respond to all comments

Essential Tools

GitHub Desktop: GUI client
GitLens: VS Code extension
gh: GitHub CLI
pre-commit: Git hooks
 Previous
Database: SQLite
Next 
2. Deployment Tools


--- 2. Deployment Tools ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Deployment Tools

Any application you build is likely to be deployed somewhere. This section covers the most popular tools involved in deploying an application.

 Previous
Version Control: Git, GitHub
Next 
Markdown


--- Images: Compression ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Images: Compression

Image compression is essential when deploying apps. Often, pages have dozens of images. Image analysis runs over thousands of images. The cost of storage and bandwidth can grow over time.

Here are things you should know when you’re compressing images:

Image dimensions are the width and height of the image in pixels. This impacts image size a lot
Lossless compression (PNG, WebP) preserves exact data
Lossy compression (JPEG, WebP) removes some data for smaller files
Vector formats (SVG) scale without quality loss
WebP is the modern standard, supporting both lossy and lossless

Here’s a rule of thumb you can use as of 2025.

Use SVG if you can (i.e. if it’s vector graphics or you can convert it to one)
Else, reduce the image to as small as you can, and save as (lossy or lossless) WebP

Common operations with Python:

from pathlib import Path
from PIL import Image
import io

async def compress_image(input_path: Path, output_path: Path, quality: int = 85) -> None:
    """Compress an image while maintaining reasonable quality."""
    with Image.open(input_path) as img:
        # Convert RGBA to RGB if needed
        if img.mode == 'RGBA':
            img = img.convert('RGB')
        # Optimize for web
        img.save(output_path, 'WEBP', quality=quality, optimize=True)

# Batch process images
paths = Path('images').glob('*.jpg')
for p in paths:
    await compress_image(p, p.with_suffix('.webp'))
Copy to clipboard
Error
Copied

Command line tools include cwebp, pngquant, jpegoptim, and ImageMagick.

# Convert to WebP
cwebp -q 85 input.png -o output.webp

# Optimize PNG
pngquant --quality=65-80 image.png

# Optimize JPEG
jpegoptim --strip-all --all-progressive --max=85 image.jpg

# Convert and resize
convert input.jpg -resize 800x600 output.jpg

# Batch convert
mogrify -format webp -quality 85 *.jpg
Copy to clipboard
Error
Copied

Watch this video on modern image formats and optimization (15 min):

Tools for image optimization:

squoosh.app: Browser-based compression
ImageOptim: GUI tool for Mac
sharp: Node.js image processing
Pillow: Python imaging library
 Previous
Markdown
Next 
Static hosting: GitHub Pages


--- Static hosting: GitHub Pages ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Static hosting: GitHub Pages

GitHub Pages is a free hosting service that turns your GitHub repository directly into a static website whenever you push it. This is useful for sharing analysis results, data science portfolios, project documentation, and more.

Common Operations:

# Create a new GitHub repo
mkdir my-site
cd my-site
git init

# Add your static content
echo "<h1>My Site</h1>" > index.html

# Push to GitHub
git add .
git commit -m "feat(pages): initial commit"
git push origin main

# Enable GitHub Pages from the main branch on the repo settings page
Copy to clipboard
Error
Copied

Best Practices:

Keep it small
Optimize images. Prefer SVG over WEBP over 8-bit PNG.
Preload critical assets like stylesheets
Avoid committing large files like datasets, videos, etc. directly. Explore Git LFS instead.

Tools:

GitHub Desktop: GUI for Git operations
GitHub CLI: Command line interface
GitHub Actions: Automation

 Previous
Images: Compression
Next 
Notebooks: Google Colab


--- Notebooks: Google Colab ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Notebooks: Google Colab

Google Colab is a free, cloud-based Jupyter notebook environment that’s become indispensable for data scientists and ML practitioners. It’s particularly valuable because it provides free access to GPUs and TPUs, and for easy sharing of code and execution results.

While Colab is excellent for prototyping and learning, its free tier has limitations - notebooks time out after 12 hours, and GPU access can be inconsistent.

Learn how to mount Google Drive for persistent storage, manage dependencies with !pip install commands, as these are common pain points when getting started.

Google Colab features you may have missed
How to mount Google Drive to Google Colab
How to take advantage of GPUs and TPUs for your ML project
 Previous
Static hosting: GitHub Pages
Next 
Serverless hosting: Vercel


--- Serverless hosting: Vercel ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Serverless hosting: Vercel

Serverless platforms let you rent a single function instead of an entire machine. They’re perfect for small web tools that don’t need to run all the time. Here are some common real-life uses:

A contact form that emails you when someone wants to hire you (runs for 2-3 seconds, a few times per day)
A tool that converts uploaded photos to black and white (runs for 5-10 seconds when someone uploads a photo)
A chatbot that answers basic questions about your business hours (runs for 1-2 seconds per question)
A newsletter sign-up that adds emails to your mailing list (runs for 1 second per sign-up)
A webhook that posts your Etsy sales to Discord (runs for 1 second whenever you make a sale)

You only pay when someone uses your tool, and the platform automatically handles busy periods. For example, if 100 people fill out your contact form at once, the platform creates 100 temporary copies of your code to handle them all. When they’re done, these copies disappear. It’s cheaper than running a full-time server because you’re not paying for the time when no one is using your tool - most tools are idle 95% of the time!

Rather than writing a full program, serverless platforms let you write functions. These functions are called via HTTP requests. They run in a cloud environment and are scaled up and down automatically. But this means you write programs in a different style. For example:

You can’t pip install packages - you have to use requirements.txt
You can’t read or write files from the file system - you can only use APIs.
You can’t run commands (e.g. subprocess.run())

Vercel is a cloud platform optimized for frontend frameworks and serverless functions. Vercel is tightly integrated with GitHub. Pushing to your repository automatically triggers new deployments.

Here’s a quickstart. Sign-up with Vercel. Create an empty git repo with this api/index.py file.

To deploy a FastAPI app, add a requirements.txt file with fastapi as a dependency.

fastapi
Copy to clipboard
Error
Copied

Add your FastAPI code to a file, e.g. main.py.

# main.py
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello, World!"}
Copy to clipboard
Error
Copied

Add a vercel.json file to the root of your repository.

{
  "builds": [{ "src": "main.py", "use": "@vercel/python" }],
  "routes": [{ "src": "/(.*)", "dest": "main.py" }]
}
Copy to clipboard
Error
Copied

On the command line, run:

npx vercel to deploy a test version
npx vercel --prod to deploy to production

Environment Variables. Use npx vercel env add to add environment variables. In your code, use os.environ.get('SECRET_KEY') to access them.

Videos

 Previous
Notebooks: Google Colab
Next 
CI/CD: GitHub Actions


--- CI/CD: GitHub Actions ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
CI/CD: GitHub Actions

GitHub Actions is a powerful automation platform built into GitHub. It helps automate your development workflow - running tests, deploying applications, updating datasets, retraining models, etc.

Understand the basics of YAML configuration files
Explore the pre-built actions from the marketplace
How to handle secrets securely
Triggering a workflow
Staying within the free tier limits
Caching dependencies to speed up workflows

Here is a sample .github/workflows/iss-location.yml that runs daily, appends the International Space Station location data into iss-location.json, and commits it to the repository.

name: Log ISS Location Data Daily

on:
  schedule:
    # Runs at 12:00 UTC (noon) every day
    - cron: "0 12 * * *"
  workflow_dispatch: # Allows manual triggering

jobs:
  collect-iss-data:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v5

      - name: Fetch ISS location data
        run: | # python
          uv run --with requests python << 'EOF'
          import requests

          data = requests.get('http://api.open-notify.org/iss-now.json').text
          with open('iss-location.jsonl', 'a') as f:
              f.write(data + '\n')
          'EOF'

      - name: Commit and push changes
        run: | # shell
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"
          git add iss-location.jsonl
          git commit -m "Update ISS position data [skip ci]" || exit 0
          git push
Copy to clipboard
Error
Copied

Tools:

GitHub CLI: Manage workflows from terminal
Super-Linter: Validate code style
Release Drafter: Automate releases
act: Run actions locally

How to handle secrets in GitHub Actions
 Previous
Serverless hosting: Vercel
Next 
Containers: Docker, Podman


--- Containers: Docker, Podman ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Containers: Docker, Podman

Docker and Podman are containerization tools that package your application and its dependencies into a standardized unit for software development and deployment.

Docker is the industry standard. Podman is compatible with Docker and has better security (and a slightly more open license). In this course, we recommend Podman but Docker works in the same way.

Initialize the container engine:

podman machine init
podman machine start
Copy to clipboard
Error
Copied

Common Operations. (You can use docker instead of podman in the same way.)

# Pull an image
podman pull python:3.11-slim

# Run a container
podman run -it python:3.11-slim

# List containers
podman ps -a

# Stop container
podman stop container_id

# Scan image for vulnerabilities
podman scan myapp:latest

# Remove container
podman rm container_id

# Remove all stopped containers
podman container prune
Copy to clipboard
Error
Copied

You can create a Dockerfile to build a container image. Here’s a sample Dockerfile that converts a Python script into a container image.

FROM python:3.11-slim
# Set working directory
WORKDIR /app
# Typically, you would use `COPY . .` to copy files from the host machine,
# but here we're just using a simple script.
RUN echo 'print("Hello, world!")' > app.py
# Run the script
CMD ["python", "app.py"]
Copy to clipboard
Error
Copied

To build, run, and deploy the container, run these commands:

# Create an account on https://hub.docker.com/ and then login
podman login docker.io

# Build and run the container
podman build -t py-hello .
podman run -it py-hello

# Push the container to Docker Hub. Replace $DOCKER_HUB_USERNAME with your Docker Hub username.
podman push py-hello:latest docker.io/$DOCKER_HUB_USERNAME/py-hello

# Push adding a specific tag, e.g. dev
TAG=dev podman push py-hello docker.io/$DOCKER_HUB_USERNAME/py-hello:$TAG
Copy to clipboard
Error
Copied

Tools:

Dive: Explore image layers
Skopeo: Work with container images
Trivy: Security scanner

Optional: For Windows, see WSL 2 with Docker getting started
 Previous
CI/CD: GitHub Actions
Next 
DevContainers: GitHub Codespaces


--- DevContainers: GitHub Codespaces ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
IDE: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
IDE: GitHub Codespaces

GitHub Codespaces is a cloud-hosted development environment built right into GitHub that gets you coding faster with pre-configured containers, adjustable compute power, and seamless integration with workflows like Actions and Copilot.

Why Codespaces helps

Reproducible onboarding: Say goodbye to “works on my machine” woes—everyone uses the same setup for assignments or demos.
Anywhere access: Jump back into your project from a laptop, tablet, or phone without having to reinstall anything.
Rapid experimentation & debugging: Spin up short-lived environments on any branch, commit, or PR to isolate bugs or test features, or keep longer-lived codespaces for big projects.

Quick Setup

From the GitHub UI

Go to your repo and click Code → Codespaces → New codespace.
Pick the branch and machine specs (2–32 cores, 8–64 GB RAM), then click Create codespace.

In Visual Studio Code

Press Ctrl+Shift+P (or Cmd+Shift+P on Mac), choose Codespaces: Create New Codespace, and follow the prompts.

Via GitHub CLI

gh auth login
gh codespace create --repo OWNER/REPO
gh codespace list    # List all codespaces
gh codespace code    # opens in your local VS Code
gh codespace ssh     # SSH into the codepsace
Copy to clipboard
Error
Copied
Features To Explore
Dev Containers: Set up your environment the same way every time using a devcontainer.json or your own Dockerfile. Introduction to dev containers
Prebuilds: Build bigger or more complex repos in advance so codespaces start up in a flash. About prebuilds
Port Forwarding: Let Codespaces spot and forward the ports your web apps use automatically. Forward ports in Codespaces
Secrets & Variables: Keep your environment variables safe in the Codespaces settings for your repo. Manage Codespaces secrets
Dotfiles Integration: Bring in your dotfiles repo to customize shell settings, aliases, and tools in every codespace. Personalizing your codespaces
Machine Types & Cost Control: Pick from VMs with 2 to 32 cores and track your usage in the billing dashboard. Managing Codespaces costs
VS Code & CLI Integration: Flip between browser VS Code and your desktop editor, and script everything with the CLI. VS Code Remote: Codespaces
GitHub Actions: Power up prebuilds and your CI/CD right inside codespaces using Actions workflows. Prebuilding your codespaces
Copilot in Codespaces: Let Copilot help you write code with in-editor AI suggestions. Copilot in Codespaces
 Previous
Containers: Docker, Podman
Next 
Tunneling: ngrok


--- Tunneling: ngrok ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Tunneling: ngrok

Ngrok is a tool that creates secure tunnels to your localhost, making your local development server accessible to the internet. It’s essential for testing webhooks, sharing work in progress, or debugging applications in production-like environments.

Run the command uvx ngrok http 8000 to create a tunnel to your local server on port 8000. This generates a public URL that you can share with others.

To get started, log into ngrok.com and get an authtoken from the dashboard. Copy it. Then run:

ngrok config add-authtoken $YOUR_AUTHTOKEN
Copy to clipboard
Error
Copied

Now you can forward any local port to the internet. For example:

# Start a local server on port 8000
uv run -m http.server 8000

# Start HTTP tunnel
uvx ngrok http 8000
Copy to clipboard
Error
Copied

Here are useful things you can do with ngrok http:

ngrok http file://. to serve local files
--response-header-add "Access-Control-Allow-Origin: *" to enable CORS
--oauth google --oauth-client-id $CLIENT_ID --oauth-client-secret $SECRET --oauth-allow-domain example.com --oauth-allow-email user@example.org to restrict users to @example.com and user@example.org using Google Auth
--ua-filter-deny ".*bot$" to reject user agents ending with bot
 Previous
DevContainers: GitHub Codespaces
Next 
CORS


--- CORS ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
CORS: Cross-Origin Resource Sharing
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
CORS: Cross-Origin Resource Sharing

CORS (Cross-Origin Resource Sharing) is a security mechanism that controls how web browsers handle requests between different origins (domains, protocols, or ports). Data scientists need CORS for APIs serving data or analysis to a browser on a different domain.

Watch this practical explanation of CORS (3 min):

Key CORS concepts:

Same-Origin Policy: Browsers block requests between different origins by default
CORS Headers: Server responses must include specific headers to allow cross-origin requests
Preflight Requests: Browsers send OPTIONS requests to check if the actual request is allowed
Credentials: Special handling required for requests with cookies or authentication

If you’re exposing your API with a GET request publicly, the only thing you need to do is set the HTTP header Access-Control-Allow-Origin: *.

Here are other common CORS headers:

Access-Control-Allow-Origin: https://example.com
Access-Control-Allow-Methods: GET, POST, PUT, DELETE
Access-Control-Allow-Headers: Content-Type, Authorization
Access-Control-Allow-Credentials: true
Copy to clipboard
Error
Copied

To implement CORS in FastAPI, use the CORSMiddleware middleware:

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()

app.add_middleware(CORSMiddleware, allow_origins=["*"]) # Allow GET requests from all origins
# Or, provide more granular control:
app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://example.com"],  # Allow a specific domain
    allow_credentials=True,  # Allow cookies
    allow_methods=["GET", "POST", "PUT", "DELETE"],  # Allow specific methods
    allow_headers=["*"],  # Allow all headers
)
Copy to clipboard
Error
Copied

Testing CORS with JavaScript:

// Simple request
const response = await fetch("https://api.example.com/data", {
  method: "GET",
  headers: { "Content-Type": "application/json" },
});

// Request with credentials
const response = await fetch("https://api.example.com/data", {
  credentials: "include",
  headers: { "Content-Type": "application/json" },
});
Copy to clipboard
Error
Copied

Useful CORS debugging tools:

CORS Checker: Test CORS configurations
Browser DevTools Network tab: Inspect CORS headers and preflight requests
cors-anywhere: CORS proxy for development

Common CORS errors and solutions:

No 'Access-Control-Allow-Origin' header: Configure server to send proper CORS headers
Request header field not allowed: Add required headers to Access-Control-Allow-Headers
Credentials flag: Set both credentials: 'include' and Access-Control-Allow-Credentials: true
Wild card error: Cannot use * with credentials; specify exact origins
 Previous
Tunneling: ngrok
Next 
REST APIs


--- REST APIs ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
REST APIs

REST (Representational State Transfer) APIs are the standard way to build web services that allow different systems to communicate over HTTP. They use standard HTTP methods and JSON for data exchange.

Watch this comprehensive introduction to REST APIs (52 min):

Key Concepts:

HTTP Methods
GET: Retrieve data
POST: Create new data
PUT/PATCH: Update existing data
DELETE: Remove data
Status Codes
2xx: Success (200 OK, 201 Created)
4xx: Client errors (400 Bad Request, 404 Not Found)
5xx: Server errors (500 Internal Server Error)

Here’s a minimal REST API using FastAPI. Run this server.py script via uv run server.py:

# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "fastapi",
#     "uvicorn",
# ]
# ///
from fastapi import FastAPI, HTTPException
from typing import Dict, List

app = FastAPI()

# Create a list of items that will act like a database
items: List[Dict[str, float | int | str]] = []

# Create a GET endpoint that returns all items
@app.get("/items")
async def get_items() -> List[Dict[str, float | int | str]]:
    return items

# Create a GET endpoint that returns a specific item by ID
@app.get("/items/{item_id}")
async def get_item(item_id: int) -> Dict[str, float | int | str]:
    if item := next((i for i in items if i["id"] == item_id), None):
        return item
    raise HTTPException(status_code=404, detail="Item not found")

# Create a POST endpoint that creates a new item
@app.post("/items")
async def create_item(item: Dict[str, float | str]) -> Dict[str, float | int | str]:
    new_item = {"id": len(items) + 1, "name": item["name"], "price": float(item["price"])}
    items.append(new_item)
    return new_item

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
Copy to clipboard
Error
Copied

Test the API with curl:

# Get all items
curl http://localhost:8000/items

# Create an item
curl -X POST http://localhost:8000/items \
  -H "Content-Type: application/json" \
  -d '{"name": "Book", "price": 29.99}'

# Get specific item
curl http://localhost:8000/items/1
Copy to clipboard
Error
Copied

Best Practices:

Use Nouns for Resources
Good: /users, /posts
Bad: /getUsers, /createPost
Version Your API
/api/v1/users
/api/v2/users
Copy to clipboard
Error
Copied
Handle Errors Consistently
{
  "error": "Not Found",
  "message": "User 123 not found",
  "status_code": 404
}
Copy to clipboard
Error
Copied
Use Query Parameters for Filtering
/api/posts?status=published&category=tech
Copy to clipboard
Error
Copied
Implement Pagination
/api/posts?page=2&limit=10
Copy to clipboard
Error
Copied

Tools:

Postman: API testing and documentation
Swagger/OpenAPI: API documentation
HTTPie: Modern command-line HTTP client
JSON Schema: API request/response validation

Learn more about REST APIs:

REST API Design Best Practices
Microsoft REST API Guidelines
Google API Design Guide
 Previous
CORS
Next 
Web Framework: FastAPI


--- Web Framework: FastAPI ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Web Framework: FastAPI

FastAPI is a modern Python web framework for building APIs with automatic interactive documentation. It’s fast, easy to use, and designed for building production-ready REST APIs.

Here’s a minimal FastAPI app, app.py:

# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "fastapi",
#   "uvicorn",
# ]
# ///

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "Hello!"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
Copy to clipboard
Error
Copied

Run this with uv run app.py.

Handle errors by raising HTTPException

from fastapi import HTTPException

async def get_item(item_id: int):
    if not valid_item(item_id):
        raise HTTPException(
            status_code=404,
            detail=f"Item {item_id} not found"
        )
Copy to clipboard
Error
Copied

Use middleware for logging

from fastapi import Request
import time

@app.middleware("http")
async def add_timing(request: Request, call_next):
    start = time.time()
    response = await call_next(request)
    response.headers["X-Process-Time"] = str(time.time() - start)
    return response
Copy to clipboard
Error
Copied

Tools:

FastAPI CLI: Project scaffolding
Pydantic: Data validation
SQLModel: SQL databases
FastAPI Users: Authentication

Watch this FastAPI Course for Beginners (64 min):

 Previous
REST APIs
Next 
Authentication: Google Auth


--- Authentication: Google Auth ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Google Authentication with FastAPI
Local LLMs: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Google Authentication with FastAPI

Secure your API endpoints using Google ID tokens to restrict access to specific email addresses.

Google Auth is the most commonly implemented single sign-on mechanism because:

It’s popular and user-friendly. Users can log in with their existing Google accounts.
It’s secure: Google supports OAuth2 and OpenID Connect to handle authentication.

Here’s how you build a FastAPI app that identifies the user.

Go to the Google Cloud Console – Credentials and click Create Credentials > OAuth client ID.

Choose Web application, set your authorized redirect URIs (e.g., http://localhost:8000/).

Copy the Client ID and Client Secret into a .env file:

GOOGLE_CLIENT_ID=your-client-id.apps.googleusercontent.com
GOOGLE_CLIENT_SECRET=your-client-secret
Copy to clipboard
Error
Copied

Create your FastAPI app.py:

# /// script
# dependencies = ["python-dotenv", "fastapi", "uvicorn", "itsdangerous", "httpx", "authlib"]
# ///

import os
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.responses import RedirectResponse
from starlette.middleware.sessions import SessionMiddleware
from authlib.integrations.starlette_client import OAuth

load_dotenv()
app = FastAPI()
app.add_middleware(SessionMiddleware, secret_key="create-a-random-secret-key")

oauth = OAuth()
oauth.register(
    name="google",
    client_id=os.getenv("GOOGLE_CLIENT_ID"),
    client_secret=os.getenv("GOOGLE_CLIENT_SECRET"),
    server_metadata_url="https://accounts.google.com/.well-known/openid-configuration",
    client_kwargs={"scope": "openid email profile"},
)

@app.get("/")
async def application(request: Request):
    user = request.session.get("user")
    # 3. For authenticated users: say hello
    if user:
        return f"Hello {user['email']}"
    # 2. For users who have just logged in, save their details in the session
    if "code" in request.query_params:
        token = await oauth.google.authorize_access_token(request)
        request.session["user"] = token["userinfo"]
        return RedirectResponse("/")
    # 1. For users who are logging in for the first time, redirect to Google login
    return await oauth.google.authorize_redirect(request, request.url)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, port=8000)
Copy to clipboard
Error
Copied

Now, run uv run app.py.

When you visit http://localhost:8000/ you’ll be redirected to a Google login page.
When you log in, you’ll be redirected back to http://localhost:8000/
Now you’ll see the email ID you logged in with.

Instead of displaying the email, you can show different content based on the user. For example:

Allow access to specfic users and not others
Fetch the user’s personalized information
Display different content based on the user
 Previous
Web Framework: FastAPI
Next 
Local LLMs: Ollama


--- Local LLMs: Ollama ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
Markdown
Images: Compression
Static hosting: GitHub Pages
Notebooks: Google Colab
Serverless hosting: Vercel
CI/CD: GitHub Actions
Containers: Docker, Podman
DevContainers: GitHub Codespaces
Tunneling: ngrok
CORS
REST APIs
Web Framework: FastAPI
Authentication: Google Auth
Local LLMs: Ollama
Local LLM Runner: Ollama
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Local LLM Runner: Ollama

ollama is a command-line tool for running open-source large language models entirely on your own machine—no API keys, no vendor lock-in, full control over models and performance.

Basic Usage

Download Ollama for macOS, Linux, or Windows and add the binary to your PATH. See the full Docs ↗ for installation details and troubleshooting.

# List installed and available models
ollama list

# Download/pin a specific model version
ollama pull gemma3:1b-it-qat

# Run a one-off prompt
ollama run gemma3:1b-it-qat 'Write a haiku about data visualization'

# Launch a persistent HTTP API on port 11434
ollama serve

# Interact programmatically over HTTP
curl -X POST http://localhost:11434/api/chat \
     -H 'Content-Type: application/json' \
     -d '{"model":"gemma3:1b-it-qat","prompt":"Hello, world!"}'
Copy to clipboard
Error
Copied
Key Features
Model management: list/pull — Install and switch among Llama 3.3, DeepSeek-R1, Gemma 3, Mistral, Phi-4, and more.
Local inference: run — Execute prompts entirely on-device for privacy and zero latency beyond hardware limits.
Persistent server: serve — Expose a local REST API for multi-session chats and integration into scripts or apps.
Version pinning: pull model:tag — Pin exact model versions for reproducible demos and experiments.
Resource control: --threads / --context — Tune CPU/GPU usage and maximum context window for performance and memory management.
Real-World Use Cases
Quick prototyping. Brainstorm slide decks or blog outlines offline, without worrying about API quotas: ollama run gemma-3 'Outline a slide deck on Agile best practices'
Data privacy. Summarize sensitive documents on-device, retaining full control of your data: cat financial_report.pdf | ollama run phi-4 'Summarize the key findings'
CI/CD integration. Validate PR descriptions or test YAML configurations in your pipeline without incurring API costs: git diff origin/main | ollama run llama2 'Check for style and clarity issues'
Local app embedding. Power a desktop or web app via the local REST API for instant LLM features: curl -X POST http://localhost:11434/api/chat -d '{"model":"mistral","prompt":"Translate to German"}'

Read the full Ollama docs ↗ for advanced topics like custom model hosting, GPU tuning, and integrating with your development workflows.

 Previous
Authentication: Google Auth
Next 
3. Large Language Models


--- 3. Large Language Models ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
AI Proxy - Jan 2025
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Large Language Models

This module covers the practical usage of large language models (LLMs).

LLMs incur a cost. For the May 2025 batch, use aipipe.org as a proxy. Emails with @ds.study.iitm.ac.in get a $1 per calendar month allowance. (Don’t exceed that.)

Read the AI Pipe documentation to learn how to use it. But in short:

Replace OPENAI_BASE_URL, i.e. https://api.openai.com/v1 with https://aipipe.org/openrouter/v1... or https://aipipe.org/openai/v1...
Replace OPENAI_API_KEY with the AIPIPE_TOKEN
Replace model names, e.g. gpt-4.1-nano, with openai/gpt-4.1-nano

For example, let’s use Gemini 2.0 Flash Lite via OpenRouter for chat completions and Text Embedding 3 Small via OpenAI for embeddings:

curl https://aipipe.org/openrouter/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $AIPIPE_TOKEN" \
  -d '{
    "model": "google/gemini-2.0-flash-lite-001",
    "messages": [{ "role": "user", "content": "What is 2 + 2?"} }]
  }'

curl https://aipipe.org/openai/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $AIPIPE_TOKEN" \
  -d '{ "model": "text-embedding-3-small", "input": "What is 2 + 2?" }'
Copy to clipboard
Error
Copied

Or using llm:

llm keys set openai --value $AIPIPE_TOKEN

export OPENAI_BASE_URL=https://aipipe.org/openrouter/v1
llm 'What is 2 + 2?' -m openrouter/google/gemini-2.0-flash-lite-001

export OPENAI_BASE_URL=https://aipipe.org/openai/v1
llm embed -c 'What is 2 + 2' -m 3-small
Copy to clipboard
Error
Copied

For a 50% discount (but slower speed), use Flex processing by adding service_tier: "flex" to your JSON request.

AI Proxy - Jan 2025

For the Jan 2025 batch, we had created API keys for everyone with an iitm.ac.in email to use gpt-4o-mini and text-embedding-3-small. Your usage is limited to $1 per calendar month for this course. Don’t exceed that.

Use AI Proxy instead of OpenAI. Specifically:

Replace your API to https://api.openai.com/... with https://aiproxy.sanand.workers.dev/openai/...
Replace the OPENAI_API_KEY with the AIPROXY_TOKEN that someone will give you.
 Previous
Local LLMs: Ollama
Next 
Prompt engineering


--- TDS TA Instructions ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
TDS TA Instructions

The TDS TA is a virtual assistant that helps you with your doubts.

It has been trained on course content created as follows:

# Clone the course repository
git clone https://github.com/sanand0/tools-in-data-science-public.git
cd tools-in-data-science-public

# Create a prompt file for the TA
PYTHONUTF8=1 uvx files-to-prompt --cxml *.md -o tds-content.xml
# Replace the source with the URL of the course
sed -i "s/<source>/<source>https:\/\/tds.s-anand.net\/#\//g" tds-content.xml
Copy to clipboard
Error
Copied

Additionally, we visit each of the evaluation links on https://exam.sanand.workers.dev/, copy it as Markdown, and add it to the content, called ga1.md, ga2.md, etc.

These files are uploaded to the IITM TDS Teaching Assistant.

Take a look at the GPT’s instructions. These were generated by the OpenAI Prompt Generation tool.

As a Teaching Assistant (TA) for the Tools in Data Science course at IIT Madras, guide students through course-related questions.

1. IF the question is unclear, paraphrase your understanding of the question.
2. Cite all relevant sections from `tds-content.xml` or `ga*.md`. Begin with: "According to [this reference](https://tds.s-anand.net/#/...), ...". Cite ONLY from the relevant <source>. ALWAYS cite verbatim. Mention ALL material relevant to the question.
3. Search online for additional answers. Share results WITH CITATION LINKS.
4. Think step-by-step. Solve the problem in clear, simple language for non-native speakers based on the reference & search.
5. Follow-up: Ask thoughtful questions to help students explore and learn.
Copy to clipboard
Error
Copied
 Previous
Prompt engineering
Next 
TDS GPT Reviewer


--- TDS GPT Reviewer ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
Content creation prompts
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
TDS GPT Reviewer

After the later parts of this course’s contents were written, we ran it through a Technical Content Reviewer GPT.

Take a look at the GPT’s instructions. These were generated by the OpenAI Prompt Generation tool.

As a **Content Reviewer** for a high school–level course on Tools in Data Science, your job is to evaluate provided content (such as text, code snippets, or references) with a focus on correctness, clarity, and conciseness, and offer actionable feedback for improvement.

1. **Check for Correctness and Consistency**
   - Verify technical and factual accuracy.
   - Ensure internal consistency without contradictions.
2. **Check for Clarity and Approachability**
   - Ensure content is understandable for a high school student with limited prior knowledge.
   - Identify and simplify jargon or advanced concepts.
3. **Check for Conciseness**
   - Assess if content is direct and free of unnecessary verbosity.
   - Identify areas for streamlining to enhance readability.
4. **Provide Feedback for Improvement**
   - Offer actionable suggestions for fixing, clarifying, or reorganizing content.
   - Propose alternative phrasing if text is vague, complex, or verbose.

# Steps

1. Carefully read the entire content before forming conclusions.
2. List factual inconsistencies or missing details causing confusion.
3. Suggest simpler terms or analogies for complex language.
4. Point out unnecessary repetition or filler text.
5. Provide direct examples of how to improve the highlighted issues.

# Output Format

Respond using **Markdown** with the following structure:

1. **Summary of Findings**
   - A concise paragraph outlining overall strengths and weaknesses.
2. **Detailed Review**
   - **Correctness and Consistency**: Note factual errors or inconsistencies, suggesting corrections.
   - **Clarity and Approachability**: Identify overly advanced or unclear sections, offering simpler alternatives.
   - **Conciseness**: Highlight long or repetitive sections with suggestions for tightening the text.
3. **Actionable Improvement Suggestions**
   - Provide specific sentences, bullet points, or rewritten examples to illustrate improvements.

# Notes

- Maintain a constructive review tone, not content generation.
- Even if content is perfect, confirm with suggestions for minor improvements (e.g., adding an example or clarifying a subtle point).
Copy to clipboard
Error
Copied
Content creation prompts

In addition, here are a few prompts used to create the content:

Video summaries. Transcribe the video via YouTube Transcript or Whisper. Then: Summarize this video transcript crisply for a high school student.
 Previous
TDS TA Instructions
Next 
LLM Sentiment Analysis


--- Content creation prompts ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
Content creation prompts
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
TDS GPT Reviewer

After the later parts of this course’s contents were written, we ran it through a Technical Content Reviewer GPT.

Take a look at the GPT’s instructions. These were generated by the OpenAI Prompt Generation tool.

As a **Content Reviewer** for a high school–level course on Tools in Data Science, your job is to evaluate provided content (such as text, code snippets, or references) with a focus on correctness, clarity, and conciseness, and offer actionable feedback for improvement.

1. **Check for Correctness and Consistency**
   - Verify technical and factual accuracy.
   - Ensure internal consistency without contradictions.
2. **Check for Clarity and Approachability**
   - Ensure content is understandable for a high school student with limited prior knowledge.
   - Identify and simplify jargon or advanced concepts.
3. **Check for Conciseness**
   - Assess if content is direct and free of unnecessary verbosity.
   - Identify areas for streamlining to enhance readability.
4. **Provide Feedback for Improvement**
   - Offer actionable suggestions for fixing, clarifying, or reorganizing content.
   - Propose alternative phrasing if text is vague, complex, or verbose.

# Steps

1. Carefully read the entire content before forming conclusions.
2. List factual inconsistencies or missing details causing confusion.
3. Suggest simpler terms or analogies for complex language.
4. Point out unnecessary repetition or filler text.
5. Provide direct examples of how to improve the highlighted issues.

# Output Format

Respond using **Markdown** with the following structure:

1. **Summary of Findings**
   - A concise paragraph outlining overall strengths and weaknesses.
2. **Detailed Review**
   - **Correctness and Consistency**: Note factual errors or inconsistencies, suggesting corrections.
   - **Clarity and Approachability**: Identify overly advanced or unclear sections, offering simpler alternatives.
   - **Conciseness**: Highlight long or repetitive sections with suggestions for tightening the text.
3. **Actionable Improvement Suggestions**
   - Provide specific sentences, bullet points, or rewritten examples to illustrate improvements.

# Notes

- Maintain a constructive review tone, not content generation.
- Even if content is perfect, confirm with suggestions for minor improvements (e.g., adding an example or clarifying a subtle point).
Copy to clipboard
Error
Copied
Content creation prompts

In addition, here are a few prompts used to create the content:

Video summaries. Transcribe the video via YouTube Transcript or Whisper. Then: Summarize this video transcript crisply for a high school student.
 Previous
TDS TA Instructions
Next 
LLM Sentiment Analysis


--- LLM Sentiment Analysis ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
LLM Sentiment Analysis

OpenAI’s API provides access to language models like GPT 4o, GPT 4o mini, etc.

For more details, read OpenAI’s guide for:

Text Generation
Vision
Structured Outputs

Start with this quick tutorial:

Here’s a minimal example using curl to generate text:

curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "messages": [{ "role": "user", "content": "Write a haiku about programming." }]
  }'
Copy to clipboard
Error
Copied

Let’s break down the request:

curl https://api.openai.com/v1/chat/completions: The API endpoint for text generation.
-H "Content-Type: application/json": The content type of the request.
-H "Authorization: Bearer $OPENAI_API_KEY": The API key for authentication.
-d: The request body.
"model": "gpt-4o-mini": The model to use for text generation.
"messages":: The messages to send to the model.
"role": "user": The role of the message.
"content": "Write a haiku about programming.": The content of the message.

This video explains how to use large language models (LLMs) for sentiment analysis and classification, covering:

Sentiment Analysis: Use OpenAI API to identify the sentiment of movie reviews as positive or negative.
Prompt Engineering: Learn how to craft effective prompts to get desired results from LLMs.
LLM Training: Understand how to train LLMs by providing examples and feedback.
OpenAI API Integration: Integrate OpenAI API into Python code to perform sentiment analysis.
Tokenization: Learn about tokenization and its impact on LLM input and cost.
Zero-Shot, One-Shot, and Multi-Shot Learning: Understand different approaches to using LLMs for learning.

Here are the links used in the video:

Jupyter Notebook
Movie reviews dataset
OpenAI Playground
OpenAI Pricing
OpenAI Tokenizer
OpenAI API Reference
OpenAI Docs
 Previous
TDS GPT Reviewer
Next 
LLM Text Extraction


--- LLM Text Extraction ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
LLM Text Extraction

JSON is one of the most widely used formats in the world for applications to exchange data.

This video explains how to use LLMs to extract structure from unstructured data, covering:

LLM for Data Extraction: Use OpenAI’s API to extract structured information from unstructured data like addresses.
JSON Schema: Define a JSON schema to ensure consistent and structured output from the LLM.
Prompt Engineering: Craft effective prompts to guide the LLM’s response and improve accuracy.
Data Cleaning: Use string functions and OpenAI’s API to clean and standardize data.
Data Analysis: Analyze extracted data using Pandas to gain insights.
LLM Limitations: Understand the limitations of LLMs, including potential errors and inconsistencies in output.
Production Use Cases: Explore real-world applications of LLMs for data extraction, such as customer service email analysis.

Here are the links used in the video:

Jupyter Notebook
JSON Schema
Function calling

Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema, so you don’t need to worry about the model omitting a required key, or hallucinating an invalid enum value.

curl https://api.openai.com/v1/chat/completions \
-H "Authorization: Bearer $OPENAI_API_KEY" \
-H "Content-Type: application/json" \
-d '{
  "model": "gpt-4o-2024-08-06",
  "messages": [
    { "role": "system", "content": "You are a helpful math tutor. Guide the user through the solution step by step." },
    { "role": "user", "content": "how can I solve 8x + 7 = -23" }
  ],
  "response_format": {
    "type": "json_schema",
    "json_schema": {
      "name": "math_response",
      "strict": true
      "schema": {
        "type": "object",
        "properties": {
          "steps": {
            "type": "array",
            "items": {
              "type": "object",
              "properties": { "explanation": { "type": "string" }, "output": { "type": "string" } },
              "required": ["explanation", "output"],
              "additionalProperties": false
            }
          },
          "final_answer": { "type": "string" }
        },
        "required": ["steps", "final_answer"],
        "additionalProperties": false
      }
    }
  }
}'
Copy to clipboard
Error
Copied

Here’s what the response_format tells OpenAI. The items marked ⚠️ are OpenAI specific requirements for the JSON schema.

"type": "json_schema": We want you to generate a JSON response that follows this schema.
"json_schema":: We’re going to give you a schema.
"name": "math_response": The schema is called math_response. (We can call it anything.)
"strict": true: Follow the schema exactly.
"schema":: Now, here’s the actual JSON schema.
"type": "object": Return an object. ⚠️ The root object must be an object.
"properties":: The object has these properties:
"steps":: There’s a steps property.
"type": "array": It’s an array.
"items":: Each item in the array…
"type": "object": … is an object.
"properties":: The object has these properties:
"explanation":: There’s an explanation property.
"type": "string": … which is a string.
"output":: There’s an output property.
"type": "string": … which is a string, too.
"required": ["explanation", "output"]: ⚠️ You must add "required": [...] and include all fields int he object.
"additionalProperties": false: ⚠️ OpenAI requires every object to have "additionalProperties": false.
"final_answer":: There’s a final_answer property.
"type": "string": … which is a string.
"required": ["steps", "final_answer"]: ⚠️ You must add "required": [...] and include all fields in the object.
"additionalProperties": false: ⚠️ OpenAI requires every object to have "additionalProperties": false.
 Previous
LLM Sentiment Analysis
Next 
Base 64 Encoding


--- Base 64 Encoding ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Base 64 Encoding

Base64 is a method to convert binary data into ASCII text. It’s essential when you need to transmit binary data through text-only channels or embed binary content in text formats.

Watch this quick explanation of how Base64 works (3 min):

Here’s how it works:

It takes 3 bytes (24 bits) and converts them into 4 ASCII characters
… using 64 characters: A-Z, a-z, 0-9, + and / (padding with = to make the length a multiple of 4)
There’s a URL-safe variant of Base64 that replaces + and / with - and _ to avoid issues in URLs
Base64 adds ~33% overhead (since every 3 bytes becomes 4 characters)

Common Python operations with Base64:

import base64

# Basic encoding/decoding
text = "Hello, World!"
# Convert text to base64
encoded = base64.b64encode(text.encode()).decode()  # SGVsbG8sIFdvcmxkIQ==
# Convert base64 back to text
decoded = base64.b64decode(encoded).decode()        # Hello, World!
# Convert to URL-safe base64
url_safe = base64.urlsafe_b64encode(text.encode()).decode()  # SGVsbG8sIFdvcmxkIQ==

# Working with binary files (e.g., images)
with open('image.png', 'rb') as f:
    binary_data = f.read()
    image_b64 = base64.b64encode(binary_data).decode()

# Data URI example (embed images in HTML/CSS)
data_uri = f"data:image/png;base64,{image_b64}"
Copy to clipboard
Error
Copied

Data URIs allow embedding binary data directly in HTML/CSS. This reduces the number of HTTP requests and also works offline. But it increases the file size.

For example, here’s an SVG image embedded as a data URI:

<img
  src="data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAzMiAzMiI+PGNpcmNsZSBjeD0iMTYiIGN5PSIxNiIgcj0iMTUiIGZpbGw9IiMyNTYzZWIiLz48cGF0aCBmaWxsPSIjZmZmIiBkPSJtMTYgNyAyIDcgNyAyLTcgMi0yIDctMi03LTctMiA3LTJaIi8+PC9zdmc+"
/>
Copy to clipboard
Error
Copied

Base64 is used in many places:

JSON: Encoding binary data in JSON payloads
Email: MIME attachments encoding
Auth: HTTP Basic Authentication headers
JWT: Encoding tokens in web authentication
SSL/TLS: PEM certificate format
SAML: Encoding assertions in SSO
Git: Encoding binary files in patches

Tools for working with Base64:

Base64 Decoder/Encoder for online encoding/decoding
data: URI Generator converts files to Data URIs
 Previous
LLM Text Extraction
Next 
Vision Models


--- Embeddings ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Embeddings: OpenAI and Local Models
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Embeddings: OpenAI and Local Models

Embedding models convert text into a list of numbers. These are like a map of text in numerical form. Each number represents a feature, and similar texts will have numbers close to each other. So, if the numbers are similar, the text they represent mean something similar.

This is useful because text similarity is important in many common problems:

Search. Find similar documents to a query.
Classification. Classify text into categories.
Clustering. Group similar items into clusters.
Anomaly Detection. Find an unusual piece of text.

You can run embedding models locally or using an API. Local models are better for privacy and cost. APIs are better for scale and quality.

Feature	Local Models	API
Privacy	High	Dependent on provider
Cost	High setup, low after that	Pay-as-you-go
Scale	Limited by local resources	Easily scales with demand
Quality	Varies by model	Typically high

The Massive Text Embedding Benchmark (MTEB) provides comprehensive comparisons of embedding models. These models are compared on several parameters, but here are some key ones to look at:

Rank. Higher ranked models have higher quality.
Memory Usage. Lower is better (for similar ranks). It costs less and is faster to run.
Embedding Dimensions. Lower is better. This is the number of numbers in the array. Smaller dimensions are cheaper to store.
Max Tokens. Higher is better. This is the number of input tokens (words) the model can take in a single input.
Look for higher scores in the columns for Classification, Clustering, Summarization, etc. based on your needs.
Local Embeddings

Here’s a minimal example using a local embedding model:

# /// script
# requires-python = "==3.12"
# dependencies = [
#   "sentence-transformers",
#   "httpx",
#   "numpy",
# ]
# ///

from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('BAAI/bge-base-en-v1.5')  # A small, high quality model

async def embed(text: str) -> list[float]:
    """Get embedding vector for text using local model."""
    return model.encode(text).tolist()

async def get_similarity(text1: str, text2: str) -> float:
    """Calculate cosine similarity between two texts."""
    emb1 = np.array(await embed(text1))
    emb2 = np.array(await embed(text2))
    return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))

async def main():
    print(await get_similarity("Apple", "Orange"))
    print(await get_similarity("Apple", "Lightning"))


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
Copy to clipboard
Error
Copied

Note the get_similarity function. It uses a Cosine Similarity to calculate the similarity between two embeddings.

OpenAI Embeddings

For comparison, here’s how to use OpenAI’s API with direct HTTP calls. Replace the embed function in the earlier script:

import os
import httpx

async def embed(text: str) -> list[float]:
    """Get embedding vector for text using OpenAI's API."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://api.openai.com/v1/embeddings",
            headers={"Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}"},
            json={"model": "text-embedding-3-small", "input": text}
        )
        return response.json()["data"][0]["embedding"]
Copy to clipboard
Error
Copied

NOTE: You need to set the OPENAI_API_KEY environment variable for this to work.

 Previous
Vision Models
Next 
Multimodal Embeddings


--- Embeddings: OpenAI and Local Models ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Embeddings: OpenAI and Local Models
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Embeddings: OpenAI and Local Models

Embedding models convert text into a list of numbers. These are like a map of text in numerical form. Each number represents a feature, and similar texts will have numbers close to each other. So, if the numbers are similar, the text they represent mean something similar.

This is useful because text similarity is important in many common problems:

Search. Find similar documents to a query.
Classification. Classify text into categories.
Clustering. Group similar items into clusters.
Anomaly Detection. Find an unusual piece of text.

You can run embedding models locally or using an API. Local models are better for privacy and cost. APIs are better for scale and quality.

Feature	Local Models	API
Privacy	High	Dependent on provider
Cost	High setup, low after that	Pay-as-you-go
Scale	Limited by local resources	Easily scales with demand
Quality	Varies by model	Typically high

The Massive Text Embedding Benchmark (MTEB) provides comprehensive comparisons of embedding models. These models are compared on several parameters, but here are some key ones to look at:

Rank. Higher ranked models have higher quality.
Memory Usage. Lower is better (for similar ranks). It costs less and is faster to run.
Embedding Dimensions. Lower is better. This is the number of numbers in the array. Smaller dimensions are cheaper to store.
Max Tokens. Higher is better. This is the number of input tokens (words) the model can take in a single input.
Look for higher scores in the columns for Classification, Clustering, Summarization, etc. based on your needs.
Local Embeddings

Here’s a minimal example using a local embedding model:

# /// script
# requires-python = "==3.12"
# dependencies = [
#   "sentence-transformers",
#   "httpx",
#   "numpy",
# ]
# ///

from sentence_transformers import SentenceTransformer
import numpy as np

model = SentenceTransformer('BAAI/bge-base-en-v1.5')  # A small, high quality model

async def embed(text: str) -> list[float]:
    """Get embedding vector for text using local model."""
    return model.encode(text).tolist()

async def get_similarity(text1: str, text2: str) -> float:
    """Calculate cosine similarity between two texts."""
    emb1 = np.array(await embed(text1))
    emb2 = np.array(await embed(text2))
    return float(np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2)))

async def main():
    print(await get_similarity("Apple", "Orange"))
    print(await get_similarity("Apple", "Lightning"))


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
Copy to clipboard
Error
Copied

Note the get_similarity function. It uses a Cosine Similarity to calculate the similarity between two embeddings.

OpenAI Embeddings

For comparison, here’s how to use OpenAI’s API with direct HTTP calls. Replace the embed function in the earlier script:

import os
import httpx

async def embed(text: str) -> list[float]:
    """Get embedding vector for text using OpenAI's API."""
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "https://api.openai.com/v1/embeddings",
            headers={"Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}"},
            json={"model": "text-embedding-3-small", "input": text}
        )
        return response.json()["data"][0]["embedding"]
Copy to clipboard
Error
Copied

NOTE: You need to set the OPENAI_API_KEY environment variable for this to work.

 Previous
Vision Models
Next 
Multimodal Embeddings


--- Multimodal Embeddings ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Multimodal Embeddings
Get API keys
Example Requests
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Multimodal Embeddings

Multimodal embeddings map text and images into the same vector space, enabling direct comparison between, say, a caption — “A cute cat” — and an image of that cat. This unified representation powers real-world applications like:

Cross-modal search (e.g. “find images of a sunset” via text queries)
Content recommendation (suggesting visually similar products to text descriptions)
Clustering & retrieval (grouping documents and their associated graphics)
Anomaly detection (spotting unusual image–text pairings)

By reducing different data types to a common numeric form, you unlock richer search, enhanced recommendations, and tighter integration of visual and textual data.

Get API keys

Below are the steps to grab a free API key for each provider.

Nomic Atlas
Sign up at the Nomic Atlas homepage: 👉 https://atlas.nomic.ai/ (Atlas | Nomic Atlas Documentation)
Once logged in, open the Dashboard and navigate to Settings → API Keys.
Click Create API Key, name it, and copy the generated key.

Set in your shell:

export NOMIC_API_KEY="your-nomic-api-key"
Copy to clipboard
Error
Copied
Jina AI
Visit the Jina AI Embeddings page: 👉 https://jina.ai/embeddings/ (Jina AI)
Click Get Started (no credit card needed) and register for a free account. Every new key comes with 1 million free tokens.
In the dashboard, go to API Key & Billing and copy your key.

Set in your shell:

export JINA_API_KEY="your-jina-api-key"
Copy to clipboard
Error
Copied
Google Vertex AI
Sign up for Google Cloud’s free tier (90 days, $300 credit): 👉 https://cloud.google.com/free (Google Cloud)
In the Cloud Console, open APIs & Services → Credentials: 👉 https://console.cloud.google.com/apis/credentials (Google Cloud)
Click Create credentials → API key, then copy the key.

Set in your shell:

export GOOGLE_API_KEY="your-google-api-key"
export PROJECT_ID="your-gcp-project-id"
Copy to clipboard
Error
Copied
Example Requests

Below are fully-workable snippets that:

Embed two texts (“A cute cat”, “A cardboard box”)
Embed two images (cat.jpg, box.png)
Send them to the respective API

Replace variables ($NOMIC_API_KEY, $JINA_API_KEY, $GOOGLE_API_KEY, $PROJECT_ID) before running.

1. Nomic Atlas

Text Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/text" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
        "model": "nomic-embed-text-v1.5",
        "task_type": "search_document",
        "texts": ["A cute cat", "A cardboard box"]
      }'
Copy to clipboard
Error
Copied

Image Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/image" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -F "model=nomic-embed-vision-v1.5" \
  -F "images=@cat.jpg" \
  -F "images=@box.png"
Copy to clipboard
Error
Copied
2. Jina AI

Jina’s unified /v1/embeddings endpoint accepts text strings and base64-encoded image bytes in one batch. (Jina AI)

curl -X POST "https://api.jina.ai/v1/embeddings" \
  -H "Authorization: Bearer $JINA_API_KEY" \
  -H "Content-Type: application/json" \
  -d "{
        \"model\": \"jina-clip-v2\",
        \"input\": [
          {\"text\":\"A cute cat\"},
          {\"text\":\"A cardboard box\"},,
          {\"image\":\"$(base64 -w 0 cat.jpg)\"},
          {\"image\":\"$(base64 -w 0 box.png)\"}
        ]
      }"
Copy to clipboard
Error
Copied
3. Google Vertex AI Multimodal Embeddings

Vertex AI’s multimodal model (multimodalembedding@001) takes JSON instances combining text and base64 image data. (Google Cloud)

curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/publishers/google/models/multimodalembedding@001:predict?key=$GOOGLE_API_KEY" \
  -d "{
        \"instances\": [
          {
            \"text\": \"A cute cat\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 cat.jpg)\"}
          },
          {
            \"text\": \"A cardboard box\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 box.png)\"}
          }
        ]
      }"
Copy to clipboard
Error
Copied

With these steps, you’re all set to explore and experiment with multimodal embeddings across text + image data—unifying your applications’ visual and linguistic understanding.

 Previous
Embeddings
Next 
Topic modeling


--- Get API keys ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Multimodal Embeddings
Get API keys
Example Requests
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Multimodal Embeddings

Multimodal embeddings map text and images into the same vector space, enabling direct comparison between, say, a caption — “A cute cat” — and an image of that cat. This unified representation powers real-world applications like:

Cross-modal search (e.g. “find images of a sunset” via text queries)
Content recommendation (suggesting visually similar products to text descriptions)
Clustering & retrieval (grouping documents and their associated graphics)
Anomaly detection (spotting unusual image–text pairings)

By reducing different data types to a common numeric form, you unlock richer search, enhanced recommendations, and tighter integration of visual and textual data.

Get API keys

Below are the steps to grab a free API key for each provider.

Nomic Atlas
Sign up at the Nomic Atlas homepage: 👉 https://atlas.nomic.ai/ (Atlas | Nomic Atlas Documentation)
Once logged in, open the Dashboard and navigate to Settings → API Keys.
Click Create API Key, name it, and copy the generated key.

Set in your shell:

export NOMIC_API_KEY="your-nomic-api-key"
Copy to clipboard
Error
Copied
Jina AI
Visit the Jina AI Embeddings page: 👉 https://jina.ai/embeddings/ (Jina AI)
Click Get Started (no credit card needed) and register for a free account. Every new key comes with 1 million free tokens.
In the dashboard, go to API Key & Billing and copy your key.

Set in your shell:

export JINA_API_KEY="your-jina-api-key"
Copy to clipboard
Error
Copied
Google Vertex AI
Sign up for Google Cloud’s free tier (90 days, $300 credit): 👉 https://cloud.google.com/free (Google Cloud)
In the Cloud Console, open APIs & Services → Credentials: 👉 https://console.cloud.google.com/apis/credentials (Google Cloud)
Click Create credentials → API key, then copy the key.

Set in your shell:

export GOOGLE_API_KEY="your-google-api-key"
export PROJECT_ID="your-gcp-project-id"
Copy to clipboard
Error
Copied
Example Requests

Below are fully-workable snippets that:

Embed two texts (“A cute cat”, “A cardboard box”)
Embed two images (cat.jpg, box.png)
Send them to the respective API

Replace variables ($NOMIC_API_KEY, $JINA_API_KEY, $GOOGLE_API_KEY, $PROJECT_ID) before running.

1. Nomic Atlas

Text Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/text" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
        "model": "nomic-embed-text-v1.5",
        "task_type": "search_document",
        "texts": ["A cute cat", "A cardboard box"]
      }'
Copy to clipboard
Error
Copied

Image Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/image" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -F "model=nomic-embed-vision-v1.5" \
  -F "images=@cat.jpg" \
  -F "images=@box.png"
Copy to clipboard
Error
Copied
2. Jina AI

Jina’s unified /v1/embeddings endpoint accepts text strings and base64-encoded image bytes in one batch. (Jina AI)

curl -X POST "https://api.jina.ai/v1/embeddings" \
  -H "Authorization: Bearer $JINA_API_KEY" \
  -H "Content-Type: application/json" \
  -d "{
        \"model\": \"jina-clip-v2\",
        \"input\": [
          {\"text\":\"A cute cat\"},
          {\"text\":\"A cardboard box\"},,
          {\"image\":\"$(base64 -w 0 cat.jpg)\"},
          {\"image\":\"$(base64 -w 0 box.png)\"}
        ]
      }"
Copy to clipboard
Error
Copied
3. Google Vertex AI Multimodal Embeddings

Vertex AI’s multimodal model (multimodalembedding@001) takes JSON instances combining text and base64 image data. (Google Cloud)

curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/publishers/google/models/multimodalembedding@001:predict?key=$GOOGLE_API_KEY" \
  -d "{
        \"instances\": [
          {
            \"text\": \"A cute cat\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 cat.jpg)\"}
          },
          {
            \"text\": \"A cardboard box\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 box.png)\"}
          }
        ]
      }"
Copy to clipboard
Error
Copied

With these steps, you’re all set to explore and experiment with multimodal embeddings across text + image data—unifying your applications’ visual and linguistic understanding.

 Previous
Embeddings
Next 
Topic modeling


--- Example Requests ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Multimodal Embeddings
Get API keys
Example Requests
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Multimodal Embeddings

Multimodal embeddings map text and images into the same vector space, enabling direct comparison between, say, a caption — “A cute cat” — and an image of that cat. This unified representation powers real-world applications like:

Cross-modal search (e.g. “find images of a sunset” via text queries)
Content recommendation (suggesting visually similar products to text descriptions)
Clustering & retrieval (grouping documents and their associated graphics)
Anomaly detection (spotting unusual image–text pairings)

By reducing different data types to a common numeric form, you unlock richer search, enhanced recommendations, and tighter integration of visual and textual data.

Get API keys

Below are the steps to grab a free API key for each provider.

Nomic Atlas
Sign up at the Nomic Atlas homepage: 👉 https://atlas.nomic.ai/ (Atlas | Nomic Atlas Documentation)
Once logged in, open the Dashboard and navigate to Settings → API Keys.
Click Create API Key, name it, and copy the generated key.

Set in your shell:

export NOMIC_API_KEY="your-nomic-api-key"
Copy to clipboard
Error
Copied
Jina AI
Visit the Jina AI Embeddings page: 👉 https://jina.ai/embeddings/ (Jina AI)
Click Get Started (no credit card needed) and register for a free account. Every new key comes with 1 million free tokens.
In the dashboard, go to API Key & Billing and copy your key.

Set in your shell:

export JINA_API_KEY="your-jina-api-key"
Copy to clipboard
Error
Copied
Google Vertex AI
Sign up for Google Cloud’s free tier (90 days, $300 credit): 👉 https://cloud.google.com/free (Google Cloud)
In the Cloud Console, open APIs & Services → Credentials: 👉 https://console.cloud.google.com/apis/credentials (Google Cloud)
Click Create credentials → API key, then copy the key.

Set in your shell:

export GOOGLE_API_KEY="your-google-api-key"
export PROJECT_ID="your-gcp-project-id"
Copy to clipboard
Error
Copied
Example Requests

Below are fully-workable snippets that:

Embed two texts (“A cute cat”, “A cardboard box”)
Embed two images (cat.jpg, box.png)
Send them to the respective API

Replace variables ($NOMIC_API_KEY, $JINA_API_KEY, $GOOGLE_API_KEY, $PROJECT_ID) before running.

1. Nomic Atlas

Text Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/text" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
        "model": "nomic-embed-text-v1.5",
        "task_type": "search_document",
        "texts": ["A cute cat", "A cardboard box"]
      }'
Copy to clipboard
Error
Copied

Image Embeddings

curl -X POST "https://api-atlas.nomic.ai/v1/embedding/image" \
  -H "Authorization: Bearer $NOMIC_API_KEY" \
  -F "model=nomic-embed-vision-v1.5" \
  -F "images=@cat.jpg" \
  -F "images=@box.png"
Copy to clipboard
Error
Copied
2. Jina AI

Jina’s unified /v1/embeddings endpoint accepts text strings and base64-encoded image bytes in one batch. (Jina AI)

curl -X POST "https://api.jina.ai/v1/embeddings" \
  -H "Authorization: Bearer $JINA_API_KEY" \
  -H "Content-Type: application/json" \
  -d "{
        \"model\": \"jina-clip-v2\",
        \"input\": [
          {\"text\":\"A cute cat\"},
          {\"text\":\"A cardboard box\"},,
          {\"image\":\"$(base64 -w 0 cat.jpg)\"},
          {\"image\":\"$(base64 -w 0 box.png)\"}
        ]
      }"
Copy to clipboard
Error
Copied
3. Google Vertex AI Multimodal Embeddings

Vertex AI’s multimodal model (multimodalembedding@001) takes JSON instances combining text and base64 image data. (Google Cloud)

curl -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  "https://us-central1-aiplatform.googleapis.com/v1/projects/$PROJECT_ID/locations/us-central1/publishers/google/models/multimodalembedding@001:predict?key=$GOOGLE_API_KEY" \
  -d "{
        \"instances\": [
          {
            \"text\": \"A cute cat\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 cat.jpg)\"}
          },
          {
            \"text\": \"A cardboard box\",
            \"image\": {\"bytesBase64Encoded\": \"$(base64 -w 0 box.png)\"}
          }
        ]
      }"
Copy to clipboard
Error
Copied

With these steps, you’re all set to explore and experiment with multimodal embeddings across text + image data—unifying your applications’ visual and linguistic understanding.

 Previous
Embeddings
Next 
Topic modeling


--- Topic modeling ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Topic Modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Topic Modeling

You’ll learn to use text embeddings to find text similarity and use that to create topics automatically from text, covering:

Embeddings: How large language models convert text into numerical representations.
Similarity Measurement: Understanding how similar embeddings indicate similar meanings.
Embedding Visualization: Using tools like Tensorflow Projector to visualize embedding spaces.
Embedding Applications: Using embeddings for tasks like classification and clustering.
OpenAI Embeddings: Using OpenAI’s API to generate embeddings for text.
Model Comparison: Exploring different embedding models and their strengths and weaknesses.
Cosine Similarity: Calculating cosine similarity between embeddings for more reliable similarity measures.
Embedding Cost: Understanding the cost of generating embeddings using OpenAI’s API.
Embedding Range: Understanding the range of values in embeddings and their significance.

Here are the links used in the video:

Jupyter Notebook
Tensorflow projector
Embeddings guide
Embeddings reference
Clustering on scikit-learn
Massive text embedding leaderboard (MTEB)
gte-large-en-v1.5 embedding model
Embeddings similarity threshold
 Previous
Multimodal Embeddings
Next 
Vector databases


--- Hybrid RAG with TypeSense ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Hybrid Retrieval Augmented Generation (Hybrid RAG) with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Hybrid Retrieval Augmented Generation (Hybrid RAG) with TypeSense

Hybrid RAG combines semantic (vector) search with traditional keyword search to improve retrieval accuracy and relevance. By mixing exact text matches with embedding-based similarity, you get the best of both worlds: precision when keywords are present, and semantic recall when phrasing varies. TypeSense makes this easy with built-in hybrid search and automatic embedding generation.

Below is a fully self-contained Hybrid RAG tutorial using TypeSense, Python, and the command line.

Install and run TypeSense

Install TypeSense.

mkdir typesense-data

docker run -p 8108:8108 \
  -v typesense-data:/data typesense/typesense:28.0 \
  --data-dir /data \
  --api-key=secret-key \
  --enable-cors
Copy to clipboard
Error
Copied
docker run: spins up a containerized TypeSense server on port 8108
-p 8108:8108 maps host port to container port.
-v typesense-data:/data mounts a Docker volume for persistence.
--data-dir /data points TypeSense at that volume.
--api-key=secret-key secures your API.
--enable-cors allows browser-based requests.

Expected output:

Docker logs showing TypeSense startup messages, such as Started Typesense API server.
Listening on http://0.0.0.0:8108.
Embed and import documents into TypeSense

Follow the steps in the RAG with the CLI tutorial to create a chunks.json that has one {id, content} JSON object per line.

TypeSense supports automatic embedding of documents. We’ll use that capability.

Save the following as addnotes.py and run it with uv run addnotes.py.

# /// script
# requires-python = ">=3.13"
# dependencies = ["httpx"]
# ///
import json
import httpx
import os

headers = {"X-TYPESENSE-API-KEY": "secret-key"}

schema = {
    "name": "notes",
    "fields": [
        {"name": "id", "type": "string", "facet": False},
        {"name": "content", "type": "string", "facet": False},
        {
            "name": "embedding",
            "type": "float[]",
            "embed": {
                "from": ["content"],
                "model_config": {
                    "model_name": "openai/text-embedding-3-small",
                    "api_key": os.getenv("OPENAI_API_KEY"),
                },
            },
        },
    ],
}

with open("chunks.json", "r") as f:
    chunks = [json.loads(line) for line in f.readlines()]

with httpx.Client() as client:
    # Create the collection
    if client.get(f"http://localhost:8108/collections/notes", headers=headers).status_code == 404:
        r = client.post("http://localhost:8108/collections", json=schema, headers=headers)

    # Embed the chunks
    result = client.post(
        "http://localhost:8108/collections/notes/documents/import?action=emplace",
        headers={**headers, "Content-Type": "text/plain"},
        data="\n".join(json.dumps(chunk) for chunk in chunks),
    )
    print(result.text)
Copy to clipboard
Error
Copied
httpx.Client: an HTTP client for Python.
Collection schema: id and content fields plus an embedding field with auto-generated embeddings from OpenAI.
Auto-embedding: the embed block instructs TypeSense to call the specified model for each document.
GET /collections/notes: checks existence.
POST /collections: creates the collection.
POST /collections/notes/documents/import?action=emplace: bulk upsert documents, embedding them on the fly.

Expected output:

A JSON summary string like {"success": X, "failed": 0} indicating how many docs were imported.
(On timeouts, re-run until all chunks are processed.)
4. Run a hybrid search and answer a question

Now, we can use a single curl against the Multi-Search endpoint to combine keyword and vector search as a hybrid search:

Q="What does the author affectionately call the => syntax?"

payload=$(jq -n --arg coll "notes" --arg q "$Q" \
  '{
     searches: [
       {
         collection: $coll,
         q:           $q,
         query_by:    "content,embedding",
         sort_by:     "_text_match:desc",
         prefix:      false,
         exclude_fields: "embedding"
       }
     ]
   }'
)
curl -s 'http://localhost:8108/multi_search' \
  -H "X-TYPESENSE-API-KEY: secret-key" \
  -d "$payload" \
  | jq -r '.results[].hits[].document.content' \
  | llm -s "${Q} - \$Answer ONLY from these notes. Cite verbatim from the notes." \
  | uvx streamdown
Copy to clipboard
Error
Copied
query_by: "content,embedding": tells TypeSense to score by both keyword and vector similarity.
sort_by: "_text_match:desc": boosts exact text hits.
exclude_fields: "embedding": keeps responses lightweight.
curl -d: posts the search request.
jq -r: extracts each hit’s content. See jq manual
llm -s and uvx streamdown: generate and stream a grounded answer.

Expected output:

The raw matched snippets printed first.
Then a concise, streamed LLM answer citing the note verbatim.
 Previous
RAG with the CLI)
Next 
Function Calling


--- Function Calling ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
Function Calling with OpenAI
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Function Calling with OpenAI

Function Calling allows Large Language Models to convert natural language into structured function calls. This is perfect for building chatbots and AI assistants that need to interact with your backend systems.

OpenAI supports Function Calling – a way for LLMs to suggest what functions to call and how.

Here’s a minimal example using Python and OpenAI’s function calling that identifies the weather in a given location.

# /// script
# requires-python = ">=3.11"
# dependencies = [
#   "httpx",
# ]
# ///

import httpx
import os
from typing import Dict, Any


def query_gpt(user_input: str, tools: list[Dict[str, Any]]) -> Dict[str, Any]:
    response = httpx.post(
        "https://api.openai.com/v1/chat/completions",
        headers={
            "Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}",
            "Content-Type": "application/json",
        },
        json={
            "model": "gpt-4o-mini",
            "messages": [{"role": "user", "content": user_input}],
            "tools": tools,
            "tool_choice": "auto",
        },
    )
    return response.json()["choices"][0]["message"]


WEATHER_TOOL = {
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get the current weather for a location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City name or coordinates"}
            },
            "required": ["location"],
            "additionalProperties": False,
        },
        "strict": True,
    },
}

if __name__ == "__main__":
    response = query_gpt("What is the weather in San Francisco?", [WEATHER_TOOL])
    print([tool_call["function"] for tool_call in response["tool_calls"]])
Copy to clipboard
Error
Copied
How to define functions

The function definition is a JSON schema with a few OpenAI specific properties. See the Supported schemas.

Here’s an example of a function definition for scheduling a meeting:

MEETING_TOOL = {
    "type": "function",
    "function": {
        "name": "schedule_meeting",
        "description": "Schedule a meeting room for a specific date and time",
        "parameters": {
            "type": "object",
            "properties": {
                "date": {
                    "type": "string",
                    "description": "Meeting date in YYYY-MM-DD format"
                },
                "time": {
                    "type": "string",
                    "description": "Meeting time in HH:MM format"
                },
                "meeting_room": {
                    "type": "string",
                    "description": "Name of the meeting room"
                }
            },
            "required": ["date", "time", "meeting_room"],
            "additionalProperties": False
        },
        "strict": True
    }
}
Copy to clipboard
Error
Copied
How to define multiple functions

You can define multiple functions by passing a list of function definitions to the tools parameter.

Here’s an example of a list of function definitions for handling employee expenses and calculating performance bonuses:

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_expense_balance",
            "description": "Get expense balance for an employee",
            "parameters": {
                "type": "object",
                "properties": {
                    "employee_id": {
                        "type": "integer",
                        "description": "Employee ID number"
                    }
                },
                "required": ["employee_id"],
                "additionalProperties": False
            },
            "strict": True
        }
    },
    {
        "type": "function",
        "function": {
            "name": "calculate_performance_bonus",
            "description": "Calculate yearly performance bonus for an employee",
            "parameters": {
                "type": "object",
                "properties": {
                    "employee_id": {
                        "type": "integer",
                        "description": "Employee ID number"
                    },
                    "current_year": {
                        "type": "integer",
                        "description": "Year to calculate bonus for"
                    }
                },
                "required": ["employee_id", "current_year"],
                "additionalProperties": False
            },
            "strict": True
        }
    }
]
Copy to clipboard
Error
Copied

Best Practices:

Use Strict Mode
Always set strict: True to ensure valid function calls
Define all required parameters
Set additionalProperties: False
Use tool choice
Set tool_choice: "required" to ensure that the model will always call one or more tools
The default is tool_choice: "auto" which means the model will choose a tool only if appropriate
Clear Descriptions
Write detailed function and parameter descriptions
Include expected formats and units
Mention any constraints or limitations
Error Handling
Validate function inputs before execution
Return clear error messages
Handle missing or invalid parameters
 Previous
Hybrid RAG with TypeSense
Next 
LLM Agents


--- LLM Agents ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Agents: Building AI Systems That Can Think and Act
LLM Image Generation
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
LLM Agents: Building AI Systems That Can Think and Act

LLM Agents are AI systems that can define and execute their own workflows to accomplish tasks. Unlike simple prompt-response patterns, agents make multiple LLM calls, use tools, and adapt their approach based on intermediate results. They represent a significant step toward more autonomous AI systems.

What Makes an Agent?

An LLM agent consists of three core components:

LLM Brain: Makes decisions about what to do next
Tools: External capabilities the agent can use (e.g., web search, code execution)
Memory: Retains context across multiple steps

Agents operate through a loop:

Observe the environment
Think about what to do
Take action using tools
Observe results
Repeat until task completion
Command-Line Agent Example

We’ve created a minimal command-line agent called llm-cmd-agent.py that:

Takes a task description from the command line
Generates code to accomplish the task
Automatically extracts and executes the code
Passes the results back to the LLM
Provides a final answer or tries again if the execution fails

Here’s how it works:

uv run llm-cmd-agent.py "list all Python files under the current directory, recursively, by size"
uv run llm-cmd-agent.py "convert the largest Markdown file to HTML"
Copy to clipboard
Error
Copied

The agent will:

Generate a shell script to list files with their sizes
Execute the script in a subprocess
Capture the output (stdout and stderr)
Pass the output back to the LLM for interpretation
Present a final answer to the user

Under the hood, the agent follows this workflow:

Initial prompt to generate a shell script
Code extraction from the LLM response
Code execution in a subprocess
Result interpretation by the LLM
Error handling and retry logic if needed

This demonstrates the core agent loop of:

Planning (generating code)
Execution (running the code)
Reflection (interpreting results)
Adaptation (fixing errors if needed)
Agent Architectures

Different agent architectures exist for different use cases:

ReAct (Reasoning + Acting): Interleaves reasoning steps with actions
Reflexion: Adds self-reflection to improve reasoning
MRKL (Modular Reasoning, Knowledge and Language): Combines neural and symbolic modules
Plan-and-Execute: Creates a plan first, then executes steps
Real-World Applications

LLM agents can be applied to various domains:

Research assistants that search, summarize, and synthesize information
Coding assistants that write, debug, and explain code
Data analysis agents that clean, visualize, and interpret data
Customer service agents that handle queries and perform actions
Personal assistants that manage schedules, emails, and tasks
Project Ideas

Here are some practical agent projects you could build:

Study buddy agent: Helps create flashcards, generates practice questions, and explains concepts
Job application assistant: Searches job listings, tailors resumes, and prepares interview responses
Personal finance agent: Categorizes expenses, suggests budgets, and identifies savings opportunities
Health and fitness coach: Creates workout plans, tracks nutrition, and provides motivation
Course project helper: Breaks down assignments, suggests resources, and reviews work
Best Practices
Clear instructions: Define the agent’s capabilities and limitations
Effective tool design: Create tools that are specific and reliable
Robust error handling: Agents should recover gracefully from failures
Memory management: Balance context retention with token efficiency
User feedback: Allow users to correct or guide the agent
Limitations and Challenges

Current LLM agents face several challenges:

Hallucination: Agents may generate false information or tool calls
Planning limitations: Complex tasks require better planning capabilities
Tool integration complexity: Each new tool adds implementation overhead
Context window constraints: Limited memory for long-running tasks
Security concerns: Tool access requires careful permission management
 Previous
Function Calling
Next 
LLM Image Generation


--- LLM Image Generation ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
Gemini Flash Experimental Image Generation and Editing APIs
OpenAI gpt-image-1 Model for Image Generation and Editing
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Gemini Flash Experimental Image Generation and Editing APIs

In March 2025, Google introduced native image generation and editing capabilities in the Gemini 2.0 Flash Experimental model. You can now generate and iteratively edit images via a single REST endpoint (Experiment with Gemini 2.0 Flash native image generation, Generate images | Gemini API | Google AI for Developers).

 (How to use Latest Gemini 2.0 Native Image Generation with API?)

Simple image generation

To generate a basic image, send a POST request to the generateContent method:

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A serene landscape of rolling hills at sunrise, digital art" }] }],
    "generationConfig": { "responseModalities": ["TEXT", "IMAGE"] }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Replace $GEMINI_API_KEY with your key. (Gemini API | Google AI for Developers)

Generation options

You can tweak the output with these generationConfig parameters:

responseModalities: Modalities to return (TEXT, IMAGE).
temperature (0.0–2.0): Controls randomness (default 1.0).
topP (0.0–1.0): Nucleus sampling threshold.
topK: Token selection cutoff.
maxOutputTokens: Max tokens for text parts.
stopSequences: Sequences to end generation.
seed: For reproducibility.
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A futuristic city skyline at dusk, neon lights" }] }],
    "generationConfig": {
      "responseModalities": ["TEXT", "IMAGE"],
      "temperature": 0.7,
      "topP": 0.9,
      "maxOutputTokens": 1024
    }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Image Generation Docs

Simple image editing

To edit an existing image, include it in the contents as inlineData (base64-encoded):

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Replace the background with a starry night sky"},
            {"inline_data": {"mime_type":"image/jpeg", "data": "'$(base64 -w 0 cat.jpg)'"}}
        ]
      }],
      "generationConfig": {"responseModalities": ["TEXT", "IMAGE"]}
    }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Image Editing Docs

Editing options

Editing requests support:

inlineData: Embed raw image bytes.
fileData: Reference public URLs.
All generationConfig options listed above.
safetySettings: Per-request safety rules.
Multi-turn edits by repeating contents in conversation history.
Costs and optimization

Gemini 2.0 Flash Experimental uses token-based billing:

Input (text/image/video): free tier, then $0.10 per 1M tokens.
Output (text/image): free tier, then $0.40 per 1M tokens.
Per-image flat cost for Pro models: ~$0.001315 /image (Gemini Developer API Pricing | Gemini API | Google AI for Developers, Solved: Re: Outdated Gemini Pro image pricing? By tile, or…).

To optimize:

Use smaller image sizes by setting responseMimeType.
Cache or reuse prompts with cachedContent.
Lower candidateCount or temperature for fewer tokens.
OpenAI gpt-image-1 Model for Image Generation and Editing

OpenAI’s GPT Image 1 (gpt-image-1) is a state-of-the-art multimodal model released on April 23, 2025, for high-fidelity image creation and editing.

Simple image generation

Use the Image Generations endpoint:

curl 'https://api.openai.com/v1/images/generations' \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "prompt": "A whimsical illustration of a cat playing chess",
    "n": 1,
    "size": "1024x1024"
  }' > image.png
Copy to clipboard
Error
Copied

(Generate Image | OpenAI API - Postman)

Generation options

Adjust these JSON parameters:

model: gpt-image-1 (default).
prompt: Text description.
n: Number of images.
size: 256x256, 512x512, or 1024x1024.
response_format: "url" (default) or "b64_json".
{
  "model": "gpt-image-1",
  "prompt": "...",
  "n": 2,
  "size": "512x512",
  "response_format": "b64_json"
}
Copy to clipboard
Error
Copied
Simple image editing

Use the Edits endpoint with an image and a mask:

curl https://api.openai.com/v1/images/edits \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "image": "data:image/png;base64,<BASE64_IMAGE>",
    "mask": "data:image/png;base64,<BASE64_MASK>",
    "prompt": "Add a rainbow in the sky above the mountains",
    "n": 1,
    "size": "1024x1024"
  }'
Copy to clipboard
Error
Copied

(curl - What’s the correct URL to test OpenAI API? - Stack Overflow)

Editing options

Editing requests accept:

image: Original image (base64 or URL).
mask: PNG mask for inpainting.
prompt: Instruction for the edit.
n, size, response_format as above.
Optional user field for attribution.
Costs and optimization

GPT Image 1 pricing (per 1M tokens): text input $5, image input $10, image output $40. Rough per-image costs:

Low quality: ~$0.02
Medium quality: ~$0.07
High quality: ~$0.19 (OpenAI’s GPT-Image-1 API — Create Stunning Images for Your Apps!, Usage of gpt-image-1 is priced per token, with … - Hacker News)

To optimize:

Choose smaller sizes (256x256).
Generate fewer images (n:1).
Use response_format:"url" to reduce payload.
Cache frequent prompts or images.
 Previous
LLM Agents
Next 
LLM Speech


--- OpenAI gpt-image-1 Model for Image Generation and Editing ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
Gemini Flash Experimental Image Generation and Editing APIs
OpenAI gpt-image-1 Model for Image Generation and Editing
LLM Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Gemini Flash Experimental Image Generation and Editing APIs

In March 2025, Google introduced native image generation and editing capabilities in the Gemini 2.0 Flash Experimental model. You can now generate and iteratively edit images via a single REST endpoint (Experiment with Gemini 2.0 Flash native image generation, Generate images | Gemini API | Google AI for Developers).

 (How to use Latest Gemini 2.0 Native Image Generation with API?)

Simple image generation

To generate a basic image, send a POST request to the generateContent method:

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A serene landscape of rolling hills at sunrise, digital art" }] }],
    "generationConfig": { "responseModalities": ["TEXT", "IMAGE"] }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Replace $GEMINI_API_KEY with your key. (Gemini API | Google AI for Developers)

Generation options

You can tweak the output with these generationConfig parameters:

responseModalities: Modalities to return (TEXT, IMAGE).
temperature (0.0–2.0): Controls randomness (default 1.0).
topP (0.0–1.0): Nucleus sampling threshold.
topK: Token selection cutoff.
maxOutputTokens: Max tokens for text parts.
stopSequences: Sequences to end generation.
seed: For reproducibility.
curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
  -H "Content-Type: application/json" \
  -X POST \
  -d '{
    "contents": [{ "parts": [{ "text": "A futuristic city skyline at dusk, neon lights" }] }],
    "generationConfig": {
      "responseModalities": ["TEXT", "IMAGE"],
      "temperature": 0.7,
      "topP": 0.9,
      "maxOutputTokens": 1024
    }
  }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Image Generation Docs

Simple image editing

To edit an existing image, include it in the contents as inlineData (base64-encoded):

curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY" \
    -H 'Content-Type: application/json' \
    -d '{
      "contents": [{
        "parts":[
            {"text": "Replace the background with a starry night sky"},
            {"inline_data": {"mime_type":"image/jpeg", "data": "'$(base64 -w 0 cat.jpg)'"}}
        ]
      }],
      "generationConfig": {"responseModalities": ["TEXT", "IMAGE"]}
    }' | jq -r '.candidates[].content.parts[] | select(.inlineData) | .inlineData.data' | base64 --decode > image.png
Copy to clipboard
Error
Copied

Image Editing Docs

Editing options

Editing requests support:

inlineData: Embed raw image bytes.
fileData: Reference public URLs.
All generationConfig options listed above.
safetySettings: Per-request safety rules.
Multi-turn edits by repeating contents in conversation history.
Costs and optimization

Gemini 2.0 Flash Experimental uses token-based billing:

Input (text/image/video): free tier, then $0.10 per 1M tokens.
Output (text/image): free tier, then $0.40 per 1M tokens.
Per-image flat cost for Pro models: ~$0.001315 /image (Gemini Developer API Pricing | Gemini API | Google AI for Developers, Solved: Re: Outdated Gemini Pro image pricing? By tile, or…).

To optimize:

Use smaller image sizes by setting responseMimeType.
Cache or reuse prompts with cachedContent.
Lower candidateCount or temperature for fewer tokens.
OpenAI gpt-image-1 Model for Image Generation and Editing

OpenAI’s GPT Image 1 (gpt-image-1) is a state-of-the-art multimodal model released on April 23, 2025, for high-fidelity image creation and editing.

Simple image generation

Use the Image Generations endpoint:

curl 'https://api.openai.com/v1/images/generations' \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "prompt": "A whimsical illustration of a cat playing chess",
    "n": 1,
    "size": "1024x1024"
  }' > image.png
Copy to clipboard
Error
Copied

(Generate Image | OpenAI API - Postman)

Generation options

Adjust these JSON parameters:

model: gpt-image-1 (default).
prompt: Text description.
n: Number of images.
size: 256x256, 512x512, or 1024x1024.
response_format: "url" (default) or "b64_json".
{
  "model": "gpt-image-1",
  "prompt": "...",
  "n": 2,
  "size": "512x512",
  "response_format": "b64_json"
}
Copy to clipboard
Error
Copied
Simple image editing

Use the Edits endpoint with an image and a mask:

curl https://api.openai.com/v1/images/edits \
  -H 'Content-Type: application/json' \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-image-1",
    "image": "data:image/png;base64,<BASE64_IMAGE>",
    "mask": "data:image/png;base64,<BASE64_MASK>",
    "prompt": "Add a rainbow in the sky above the mountains",
    "n": 1,
    "size": "1024x1024"
  }'
Copy to clipboard
Error
Copied

(curl - What’s the correct URL to test OpenAI API? - Stack Overflow)

Editing options

Editing requests accept:

image: Original image (base64 or URL).
mask: PNG mask for inpainting.
prompt: Instruction for the edit.
n, size, response_format as above.
Optional user field for attribution.
Costs and optimization

GPT Image 1 pricing (per 1M tokens): text input $5, image input $10, image output $40. Rough per-image costs:

Low quality: ~$0.02
Medium quality: ~$0.07
High quality: ~$0.19 (OpenAI’s GPT-Image-1 API — Create Stunning Images for Your Apps!, Usage of gpt-image-1 is priced per token, with … - Hacker News)

To optimize:

Choose smaller sizes (256x256).
Generate fewer images (n:1).
Use response_format:"url" to reduce payload.
Cache frequent prompts or images.
 Previous
LLM Agents
Next 
LLM Speech


--- LLM Speech ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
OpenAI TTS-1 for Text-to-Speech Generation
Google Gemini Speech Studio for Text-to-Speech
LLM Evals
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
OpenAI TTS-1 for Text-to-Speech Generation

OpenAI’s Text-to-Speech API (TTS-1) converts text into natural-sounding speech using state-of-the-art neural models. Released in March 2025, it offers multiple voices and control over speaking style and speed.

Simple speech generation

To generate speech from text, send a POST request to the speech endpoint:

curl https://api.openai.com/v1/audio/speech \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tts-1",
    "input": "Hello! This is a test of the OpenAI text to speech API.",
    "voice": "alloy"
  }' --output speech.mp3
Copy to clipboard
Error
Copied
Generation options

Control the output with these parameters:

model: tts-1 (standard) or tts-1-hd (higher quality)
input: Text to convert (max 4096 characters)
voice: alloy, echo, fable, onyx, nova, or shimmer
response_format: mp3 (default), opus, aac, or flac
speed: 0.25 to 4.0 (default 1.0)
curl https://api.openai.com/v1/audio/speech \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tts-1-hd",
    "input": "Welcome to our podcast! Today we will be discussing artificial intelligence.",
    "voice": "nova",
    "response_format": "mp3",
    "speed": 1.2
  }' --output podcast_intro.mp3
Copy to clipboard
Error
Copied
Costs and optimization

Pricing per 1,000 characters:

tts-1: $0.015
tts-1-hd: $0.030

To optimize costs:

Use tts-1 for drafts, tts-1-hd for final versions
Batch process text in chunks
Cache frequently used phrases
Choose lower quality formats for testing
Google Gemini Speech Studio for Text-to-Speech

Google’s Gemini Speech Studio offers advanced text-to-speech capabilities with support for multiple languages, voices, and speaking styles.

For this, you need a GOOGLE_API_KEY. You can:

Go to the Google Cloud Console: https://console.cloud.google.com/apis/library/texttospeech.googleapis.com, select or create the project you want and click Enable.
Create an API key. In the Console, navigate to APIs & Services → Credentials and click + Create Credentials → API key. Copy the generated key (it’ll look like AIza…).
Simple speech generation

Generate speech using the Gemini API:

curl -X POST "https://texttospeech.googleapis.com/v1/text:synthesize?key=$GOOGLE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": { "text": "Hello, welcome to Gemini Speech Studio!" },
    "voice": { "languageCode": "en-US", "name": "en-US-Neural2-A" },
    "audioConfig": { "audioEncoding": "MP3" }
  }' | jq -r .audioContent | base64 --decode > gemini-speech.mp3
Copy to clipboard
Error
Copied
Generation options

Customize synthesis with these parameters:

voice:
languageCode: Language code (e.g., “en-US”, “es-ES”)
name: Voice model name
ssmlGender: “NEUTRAL”, “MALE”, or “FEMALE”
audioConfig:
audioEncoding: “MP3”, “WAV”, “OGG_OPUS”
speakingRate: 0.25 to 4.0
pitch: -20.0 to 20.0
volumeGainDb: Volume adjustment
curl -X POST "https://texttospeech.googleapis.com/v1/text:synthesize?key=$GOOGLE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "text": "This is a demonstration of advanced speech settings."
    },
    "voice": {
      "languageCode": "en-US",
      "name": "en-US-Neural2-D"
    },
    "audioConfig": {
      "audioEncoding": "MP3",
      "speakingRate": 1.2,
      "pitch": 2.0,
      "volumeGainDb": 1.0
    }
  }' | jq -r .audioContent | base64 --decode > gemini-options.mp3
Copy to clipboard
Error
Copied
SSML support

Both APIs support Speech Synthesis Markup Language (SSML) for fine-grained control:

curl -X POST "https://texttospeech.googleapis.com/v1/text:synthesize?key=$GOOGLE_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "ssml": "<speak>Hello <break time=\"1s\"/> This text has a pause and <emphasis level=\"strong\">emphasized words</emphasis>.</speak>"
    },
    "voice": { "languageCode": "en-US", "name": "en-US-Neural2-A" },
    "audioConfig": { "audioEncoding": "MP3" }
  }' | jq -r .audioContent | base64 --decode > gemini-ssml.mp3
Copy to clipboard
Error
Copied
Costs and optimization

Pricing (per character):

Standard voices: $0.000004
Neural voices: $0.000016
Neural2 voices: $0.000024

To optimize:

Use standard voices for development
Cache common phrases
Batch process where possible
Choose appropriate audio quality
Python implementation

Here’s a simple Python wrapper for both APIs:

import requests
import base64
import os

openai_key = os.getenv("OPENAI_API_KEY")
google_key = os.getenv("GOOGLE_API_KEY")

def generate_openai_speech(text, voice="alloy", model="tts-1"):
    response = requests.post(
        "https://api.openai.com/v1/audio/speech",
        headers={"Authorization": f"Bearer {openai_key}"},
        json={"model": model, "input": text, "voice": voice}
    )
    return response.content

def generate_gemini_speech(text, voice_name="en-US-Neural2-A"):
    response = requests.post(
        f"https://texttospeech.googleapis.com/v1/text:synthesize?key={google_key}",
        json={
            "input": {"text": text},
            "voice": {"languageCode": "en-US", "name": voice_name},
            "audioConfig": {"audioEncoding": "MP3"}
        }
    )
    return base64.b64decode(response.json()["audioContent"])

if __name__ == "__main__":
    with open("openai_speech.mp3", "wb") as f:
        f.write(generate_openai_speech("Hello from OpenAI's text to speech API!"))
    with open("gemini_speech.mp3", "wb") as f:
        f.write(generate_gemini_speech("Hello from Google's Gemini Speech Studio!"))
Copy to clipboard
Error
Copied
 Previous
LLM Image Generation
Next 
LLM Evals


--- LLM Evals ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Prompt engineering
TDS TA Instructions
TDS GPT Reviewer
LLM Sentiment Analysis
LLM Text Extraction
Base 64 Encoding
Vision Models
Embeddings
Multimodal Embeddings
Topic modeling
Vector databases
RAG with the CLI)
Hybrid RAG with TypeSense
Function Calling
LLM Agents
LLM Image Generation
LLM Speech
LLM Evals
LLM Evaluations with PromptFoo
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
LLM Evaluations with PromptFoo

Test-drive your prompts and models with automated, reliable evaluations.

PromptFoo is a test-driven development framework for LLMs:

Developer-first: Fast CLI with live reload & caching (promptfoo.dev)
Multi-provider: Works with OpenAI, Anthropic, HuggingFace, Ollama & more (GitHub)
Assertions: Built‑in (contains, equals) & model‑graded (llm-rubric) (docs)
CI/CD: Integrate evals into pipelines for regression safety (CI/CD guide)

To run PromptFoo:

Install Node.js & npm (nodejs.org)
Set up your OPENAI_API_KEY environment variable
Configure promptfooconfig.yaml. Below is an example:
prompts:
  - |
    Summarize this text: "{{text}}"
  - |
    Please write a concise summary of: "{{text}}"

providers:
  - openai:gpt-3.5-turbo
  - openai:gpt-4

tests:
  - name: summary_test
    vars:
      text: "PromptFoo is an open-source CLI and library for evaluating and testing LLMs with assertions, caching, and matrices."
    assertions:
      - contains-all:
          values:
            - "open-source"
            - "LLMs"
      - llm-rubric:
          instruction: |
            Score the summary from 1 to 5 for:
            - relevance: captures the main info?
            - clarity: wording is clear and concise?
          schema:
            type: object
            properties:
              relevance:
                type: number
                minimum: 1
                maximum: 5
              clarity:
                type: number
                minimum: 1
                maximum: 5
            required: [relevance, clarity]
            additionalProperties: false

commandLineOptions:
  cache: true
Copy to clipboard
Error
Copied

Now, you can run the evaluations and see the results.

# Execute all tests
npx -y promptfoo eval -c promptfooconfig.yaml

# List past evaluations
npx -y promptfoo list evals

# Launch interactive results viewer on port 8080
npx -y promptfoo view -p 8080
Copy to clipboard
Error
Copied

PromptFoo caches API responses by default (TTL 14 days). You can disable it with --no-cache or clear it.

# Disable cache for this run
echo y | promptfoo eval --no-cache -c promptfooconfig.yaml

# Clear all cache
echo y | promptfoo cache clear
Copy to clipboard
Error
Copied
 Previous
LLM Speech
Next 
Project 1


--- 4. Data Sourcing ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Data Sourcing

Before you do any kind of data science, you obviously have to get the data to be able to analyze it, visualize it, narrate it, and deploy it. And what we are going to cover in this module is how you get the data.

There are three ways you can get the data.

The first is you can download the data. Either somebody gives you the data and says download it from here, or you are asked to download it from the internet because it’s a public data source. But that’s the first way—you download the data.
The second way is you can query it from somewhere. It may be on a database. It may be available through an API. It may be available through a library. But these are ways in which you can selectively query parts of the data and stitch it together.
The third way is you have to scrape it. It’s not directly available in a convenient form that you can query or download. But it is, in fact, on a web page. It’s available on a PDF file. It’s available in a Word document. It’s available on an Excel file. It’s kind of structured, but you will have to figure out that structure and extract it from there.

In this module, we will be looking at the tools that will help you either download from a data source or query from an API or from a database or from a library. And finally, how you can scrape from different sources.

Here are links used in the video:

The Movies Dataset
IMDb Datasets
Download the IMDb Datasets
Explore the Internet Movie Database
What does the world search for?
HowStat - Cricket statistics
Cricket Strike Rates
 Previous
Project 1
Next 
Scraping with Excel


--- Scraping with Google Sheets ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Scraping with Google Sheets

You’ll learn how to import tables on the web using Google Sheets’s =IMPORTHTML() formula, covering:

Import HTML Formula: Use =IMPORTHTML(URL, “query”, index) to fetch tables or lists from a web page.
Granting Access: Allow access for formulas to fetch data from external sources.
Checking Imported Data: Verify if the imported table matches the data on the web page.
Handling Errors: Understand common issues and how to resolve them.
Sorting Data: Copy imported data as values and sort it within Google Sheets.
Freezing Rows: Use frozen rows to maintain headers while sorting.
Live Formulas: Learn how web data updates automatically when the source changes.
Other Import Functions: IMPORTXML, IMPORTFEED, IMPORTRANGE, and IMPORTDATA for advanced data fetching options.

Here are links used in the video:

Google sheet used in the video
IMPORTHTML()
IMPORTXML()
Demographics of India
List of highest grossing Indian films
 Previous
Scraping with Excel
Next 
Crawling with the CLI


--- Crawling with the CLI ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Crawling with the CLI

Since websites are a common source of data, we often download entire websites (crawling) and then process them offline.

Web crawling is essential in many data-driven scenarios:

Data mining and analysis: Gathering structured data from multiple pages for market research, competitive analysis, or academic research
Content archiving: Creating offline copies of websites for preservation or backup purposes
SEO analysis: Analyzing site structure, metadata, and content to improve search rankings
Legal compliance: Capturing website content for regulatory or compliance documentation
Website migration: Creating a complete copy before moving to a new platform or design
Offline access: Downloading educational resources, documentation, or reference materials for use without internet connection

The most commonly used tool for fetching websites is wget. It is pre-installed in many UNIX distributions and easy to install.

To crawl the IIT Madras Data Science Program website for example, you could run:

wget \
  --recursive \
  --level=3 \
  --no-parent \
  --convert-links \
  --adjust-extension \
  --compression=auto \
  --accept html,htm \
  --directory-prefix=./ds \
  https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied

Here’s what each option does:

--recursive: Enables recursive downloading (following links)
--level=3: Limits recursion depth to 3 levels from the initial URL
--no-parent: Restricts crawling to only URLs below the initial directory
--convert-links: Converts all links in downloaded documents to work locally
--adjust-extension: Adds proper extensions to files (.html, .jpg, etc.) based on MIME types
--compression=auto: Automatically handles compressed content (gzip, deflate)
--accept html,htm: Only downloads files with these extensions
--directory-prefix=./ds: Saves all downloaded files to the specified directory

wget2 is a better version of wget and supports HTTP2, parallel connections, and only updates modified sites. The syntax is (mostly) the same.

wget2 \
  --recursive \
  --level=3 \
  --no-parent \
  --convert-links \
  --adjust-extension \
  --compression=auto \
  --accept html,htm \
  --directory-prefix=./ds \
  https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied

There are popular free and open-source alternatives to Wget:

Wpull

Wpull is a wget‐compatible Python crawler that supports on-disk resumption, WARC output, and PhantomJS integration.

uvx wpull \
  --recursive \
  --level=3 \
  --no-parent \
  --convert-links \
  --adjust-extension \
  --compression=auto \
  --accept html,htm \
  --directory-prefix=./ds \
  https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied
HTTrack

HTTrack is dedicated website‐mirroring tool with rich filtering and link‐conversion options.

httrack "https://study.iitm.ac.in/ds/" \
  -O "./ds" \
  "+*.study.iitm.ac.in/ds/*" \
  -r3
Copy to clipboard
Error
Copied
Robots.txt

robots.txt is a standard file found in a website’s root directory that specifies which parts of the site should not be accessed by web crawlers. It’s part of the Robots Exclusion Protocol, an ethical standard for web crawling.

Why it’s important:

Server load protection: Prevents excessive traffic that could overload servers
Privacy protection: Keeps sensitive or private content from being indexed
Legal compliance: Respects website owners’ rights to control access to their content
Ethical web citizenship: Shows respect for website administrators’ wishes

How to override robots.txt restrictions:

wget, wget2: Use -e robots=off
httrack: Use -s0
wpull: Use --no-robots

When to override robots.txt (use with discretion):

Only bypass robots.txt when:

You have explicit permission from the website owner
You’re crawling your own website
The content is publicly accessible and your crawling won’t cause server issues
You’re conducting authorized security testing

Remember that bypassing robots.txt without legitimate reason may:

Violate terms of service
Lead to IP banning
Result in legal consequences in some jurisdictions
Cause reputation damage to your organization

Always use the minimum necessary crawling speed and scope, and consider contacting website administrators for permission when in doubt.

 Previous
Scraping with Google Sheets
Next 
BBC Weather API with Python


--- BBC Weather API with Python ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
BBC Weather location ID with Python
BBC Weather data with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
BBC Weather location ID with Python

You’ll learn how to get the location ID of any city from the BBC Weather API – as a precursor to scraping weather data – covering:

Understanding API Calls: Learn how backend API calls work when searching for a city on the BBC weather website.
Inspecting Web Interactions: Use the browser’s inspect element feature to track API calls and understand the network activity.
Extracting Location IDs: Identify and extract the location ID from the API response using Python.
Using Python Libraries: Import and use requests, json, and urlencode libraries to make API calls and process responses.
Constructing API URLs: Create structured API URLs dynamically with constant prefixes and query parameters using urlencode.
Building Functions: Develop a Python function that accepts a city name, constructs the API call, and returns the location ID.

To open the browser Developer Tools on Chrome, Edge, or Firefox, you can:

Right-click on the page and select “Inspect” to open the developer tools
OR: Press F12
OR: Press Ctrl+Shift+I on Windows
OR: Press Cmd+Opt+I on Mac

Here are links and references:

BBC Location ID scraping - Notebook
BBC Weather - Palo Alto (location ID: 5380748)
BBC Locator Service - Los Angeles
Learn about the requests package. Watch Python Requests Tutorial: Request Web Pages, Download Images, POST Data, Read JSON, and More
BBC Weather data with Python

You’ll learn how to scrape the live weather data of a city from the BBC Weather API, covering:

Introduction to Web Scraping: Understand the basics of web scraping and its legality.
Libraries Overview: Learn the importance of requests and BeautifulSoup.
Fetching HTML: Use requests to fetch HTML content from a web page.
Parsing HTML: Utilize BeautifulSoup to parse and navigate the HTML content.
Identifying Data: Inspect HTML elements to locate specific data (e.g., high and low temperatures).
Extracting Data: Extract relevant data using BeautifulSoup‘s find_all() function.
Data Cleanup: Clean extracted data to remove unwanted elements.
Post-Processing: Use regular expressions to split large strings into meaningful parts.
Data Structuring: Combine extracted data into a structured pandas DataFrame.
Handling Special Characters: Replace unwanted characters for better data manipulation.
Saving Data: Save the cleaned data into CSV and Excel formats.

Here are links and references:

BBC Weather scraping - Notebook
BBC Locator Service - Mumbai
BBC Weather - Mumbai (location ID: 1275339)
BBC Weather API - Mumbai (location ID: 1275339)
Learn about the json package. Watch Python Tutorial: Working with JSON Data using the json Module
Learn about the BeautifulSoup package. Watch Python Tutorial: Web Scraping with BeautifulSoup and Requests
Learn about the pandas package. Watch
Python Pandas Tutorial (Part 1): Getting Started with Data Analysis - Installation and Loading Data
Python Pandas Tutorial (Part 2): DataFrame and Series Basics - Selecting Rows and Columns
Learn about the re package. Watch Python Tutorial: re Module - How to Write and Match Regular Expressions (Regex)
Learn about the datetime package. Watch Python Tutorial: Datetime Module - How to work with Dates, Times, Timedeltas, and Timezones
 Previous
Crawling with the CLI
Next 
Scraping IMDb with JavaScript


--- BBC Weather data with Python ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
BBC Weather location ID with Python
BBC Weather data with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
BBC Weather location ID with Python

You’ll learn how to get the location ID of any city from the BBC Weather API – as a precursor to scraping weather data – covering:

Understanding API Calls: Learn how backend API calls work when searching for a city on the BBC weather website.
Inspecting Web Interactions: Use the browser’s inspect element feature to track API calls and understand the network activity.
Extracting Location IDs: Identify and extract the location ID from the API response using Python.
Using Python Libraries: Import and use requests, json, and urlencode libraries to make API calls and process responses.
Constructing API URLs: Create structured API URLs dynamically with constant prefixes and query parameters using urlencode.
Building Functions: Develop a Python function that accepts a city name, constructs the API call, and returns the location ID.

To open the browser Developer Tools on Chrome, Edge, or Firefox, you can:

Right-click on the page and select “Inspect” to open the developer tools
OR: Press F12
OR: Press Ctrl+Shift+I on Windows
OR: Press Cmd+Opt+I on Mac

Here are links and references:

BBC Location ID scraping - Notebook
BBC Weather - Palo Alto (location ID: 5380748)
BBC Locator Service - Los Angeles
Learn about the requests package. Watch Python Requests Tutorial: Request Web Pages, Download Images, POST Data, Read JSON, and More
BBC Weather data with Python

You’ll learn how to scrape the live weather data of a city from the BBC Weather API, covering:

Introduction to Web Scraping: Understand the basics of web scraping and its legality.
Libraries Overview: Learn the importance of requests and BeautifulSoup.
Fetching HTML: Use requests to fetch HTML content from a web page.
Parsing HTML: Utilize BeautifulSoup to parse and navigate the HTML content.
Identifying Data: Inspect HTML elements to locate specific data (e.g., high and low temperatures).
Extracting Data: Extract relevant data using BeautifulSoup‘s find_all() function.
Data Cleanup: Clean extracted data to remove unwanted elements.
Post-Processing: Use regular expressions to split large strings into meaningful parts.
Data Structuring: Combine extracted data into a structured pandas DataFrame.
Handling Special Characters: Replace unwanted characters for better data manipulation.
Saving Data: Save the cleaned data into CSV and Excel formats.

Here are links and references:

BBC Weather scraping - Notebook
BBC Locator Service - Mumbai
BBC Weather - Mumbai (location ID: 1275339)
BBC Weather API - Mumbai (location ID: 1275339)
Learn about the json package. Watch Python Tutorial: Working with JSON Data using the json Module
Learn about the BeautifulSoup package. Watch Python Tutorial: Web Scraping with BeautifulSoup and Requests
Learn about the pandas package. Watch
Python Pandas Tutorial (Part 1): Getting Started with Data Analysis - Installation and Loading Data
Python Pandas Tutorial (Part 2): DataFrame and Series Basics - Selecting Rows and Columns
Learn about the re package. Watch Python Tutorial: re Module - How to Write and Match Regular Expressions (Regex)
Learn about the datetime package. Watch Python Tutorial: Datetime Module - How to work with Dates, Times, Timedeltas, and Timezones
 Previous
Crawling with the CLI
Next 
Scraping IMDb with JavaScript


--- Scraping IMDb with JavaScript ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Scraping IMDb with JavaScript

You’ll learn how to scrape the IMDb Top 250 movies directly in the browser using JavaScript on the Chrome DevTools, covering:

Access Developer Tools: Use F12 or right-click > Inspect to open developer tools in Chrome or Edge.
Inspect Elements: Identify and inspect HTML elements using the Elements tab.
Query Selectors: Use document.querySelectorAll and document.querySelector to find elements by CSS class.
Extract Text Content: Retrieve text content from elements using JavaScript.
Functional Programming: Apply map and arrow functions for concise data processing.
Data Structuring: Collect and format data into an array of arrays.
Copying Data: Use the copy function to transfer data to the clipboard.
Convert to Spreadsheet: Use online tools to convert JSON data to CSV or Excel format.
Text Manipulation: Perform text splitting and cleaning in Excel for final data formatting.

Here are links and references:

IMDB Top 250 movies
Learn about Chrome Devtools
 Previous
BBC Weather API with Python
Next 
Nominatim API with Python


--- Wikipedia Data with Python ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Wikipedia Data with Python

You’ll learn how to scrape data from Wikipedia using the wikipedia Python library, covering:

Installing and Importing: Use pip install to get the Wikipedia library and import it with import wikipedia as wk.
Keyword Search: Use the search function to find Wikipedia pages containing a specific keyword, limiting results with the results argument.
Fetching Summaries: Use the summary function to get a concise summary of a Wikipedia page, limiting sentences with the sentences argument.
Retrieving Full Pages: Use the page function to obtain the full content of a Wikipedia page, including sections and references.
Accessing URLs: Retrieve the URL of a Wikipedia page using the url attribute of the page object.
Extracting References: Use the references attribute to get all reference links from a Wikipedia page.
Fetching Images: Access all images on a Wikipedia page via the images attribute, which returns a list of image URLs.
Extracting Tables: Use the pandas.read_html function to extract tables from the HTML content of a Wikipedia page, being mindful of table indices.

Here are links and references:

Wikipedia Library - Notebook
Learn about the wikipedia package

NOTE: Wikipedia is constantly edited. The page may be different now from when the video was recorded. Handle accordingly.

 Previous
Nominatim API with Python
Next 
Scraping PDFs with Tabula


--- Scraping PDFs with Tabula ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Scraping PDFs with Tabula

You’ll learn how to scrape tables from PDFs using the tabula Python library, covering:

Import Libraries: Use Beautiful Soup for URL parsing and Tabula for extracting tables from PDFs.
Specify Save Location: Mount Google Drive to save scraped PDFs.
Identify PDF URLs: Parse the given URL to identify and select all PDF links.
Download PDFs: Loop through identified links, saving each PDF to the specified location.
Extract Tables: Use Tabula to read tabular content from the downloaded PDFs.
Control Extraction Area: Specify page and area coordinates to accurately extract tables, avoiding extraneous text.
Save Extracted Data: Convert the extracted table data into structured formats like CSV for further analysis.

Here are links and references:

PDF Scraping - Notebook
Learn about the tabula package
Learn about the pandas package. Video
 Previous
Wikipedia Data with Python
Next 
Convert PDFs to Markdown


--- Convert PDFs to Markdown ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- Markitdown ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- GROBID ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- Azure Document Intelligence API ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- Comparison of PDF-to-Markdown Tools ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- Tips for Optimal PDF Conversion ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Converting PDFs to Markdown
Markitdown
GROBID
Azure Document Intelligence API
Comparison of PDF-to-Markdown Tools
Tips for Optimal PDF Conversion
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting PDFs to Markdown

PDF documents are ubiquitous in academic, business, and technical contexts, but extracting and repurposing their content can be challenging. This tutorial explores various command-line tools for converting PDFs to Markdown format, with a focus on preserving structure and formatting suitable for different use cases, including preparation for Large Language Models (LLMs).

Use Cases:

LLM training and fine-tuning: Create clean text data from PDFs for AI model training
Knowledge base creation: Transform PDFs into searchable, editable markdown documents
Content repurposing: Convert academic papers and reports for web publication
Data extraction: Pull structured content from PDF documents for analysis
Accessibility: Convert PDFs to more accessible formats for screen readers
Citation and reference management: Extract bibliographic information from academic papers
Documentation conversion: Transform technical PDFs into maintainable documentation
PyMuPDF4LLM

PyMuPDF4LLM is a specialized component of the PyMuPDF library that generates Markdown specifically formatted for Large Language Models. It produces high-quality markdown with good preservation of document structure. It’s specifically optimized for producing text that works well with LLMs, removing irrelevant formatting while preserving semantic structure. Requires PyTorch, which adds dependencies but enables more advanced processing capabilities.

PyMuPDF4LLM uses MuPDF as its PDF parsing engine. PyMuPDF is emerging as a strong default for PDF text extraction due to its accuracy and performance in handling complex PDF structures.

PYTHONUTF8=1 uv run --with pymupdf4llm python -c 'import pymupdf4llm; h = open("pymupdf4llm.md", "w"); h.write(pymupdf4llm.to_markdown("$FILE.pdf"))'
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding regardless of system locale
uv run --with pymupdf4llm: Uses uv package manager to run Python with the pymupdf4llm package
python -c '...': Executes Python code directly from the command line
import pymupdf4llm: Imports the PDF-to-Markdown module
h = open("pymupdf4llm.md", "w"): Creates a file to write the markdown output
h.write(pymupdf4llm.to_markdown("$FILE.pdf")): Converts the PDF to markdown and writes to file
Markitdown

Markitdown is Microsoft’s tool for converting various document formats to Markdown, including PDFs, DOCX, XLSX, PPTX, and ZIP files. It’s a versatile multi-format converter that handles PDFs via PDFMiner, DOCX via Mammoth, XLSX via Pandas, and PPTX via Python-PPTX. Good for batch processing of mixed document types. The quality of PDF conversion is generally good but may struggle with complex layouts or heavily formatted documents.

PYTHONUTF8=1 uvx markitdown $FILE.pdf > markitdown.md
Copy to clipboard
Error
Copied
PYTHONUTF8=1: Forces Python to use UTF-8 encoding
uvx markitdown: Runs the markitdown tool via the uv package manager
$FILE.pdf: The input PDF file
> markitdown.md: Redirects output to a markdown file
Unstructured

Unstructured is rapidly becoming the de facto library for parsing over 40 different file types. It is excellent for extracting text and tables from diverse document formats. Particularly useful for generating clean content to pass to LLMs. Strong community support and actively maintained.

GROBID

If you specifically need to parse references from text-native PDFs or reliably OCR’ed ones, GROBID remains the de facto choice. It excels at extracting structured bibliographic information with high accuracy.

# Start GROBID service
docker run -t --rm -p 8070:8070 lfoppiano/grobid:0.7.2

# Process PDF with curl
curl -X POST -F "input=@paper.pdf" localhost:8070/api/processFulltextDocument > references.tei.xml
Copy to clipboard
Error
Copied
Mistral OCR API

Mistral OCR offers an end-to-end cloud API that preserves both text and layout, making it easier to isolate specific sections like References. It shows the most promise currently, though it requires post-processing.

Azure Document Intelligence API

For enterprise users already in the Microsoft ecosystem, Azure Document Intelligence provides excellent raw OCR with enterprise SLAs. May require custom model training or post-processing to match GROBID’s reference extraction capabilities.

Other libraries

Docling is IBM’s document understanding library that supports PDF conversion. It can be challenging to install, particularly on Windows and some Linux distributions. Offers advanced document understanding capabilities beyond simple text extraction.

MegaParse takes a comprehensive approach using LibreOffice, Pandoc, Tesseract OCR, and other tools. It has Robust handling of different document types but requires an OpenAI API key for some features. Good for complex documents but has significant dependencies.

Comparison of PDF-to-Markdown Tools
Tool	Strengths	Weaknesses	Best For
PyMuPDF4LLM	Structure preservation, LLM optimization	Requires PyTorch	AI training data, semantic structure
Markitdown	Multi-format support, simple usage	Less precise layout handling	Batch processing, mixed documents
Unstructured	Wide format support, active development	Can be resource-intensive	Production pipelines, integration
GROBID	Reference extraction excellence	Narrower use case	Academic papers, citations
Docling	Advanced document understanding	Installation difficulties	Research applications
MegaParse	Comprehensive approach	Requires OpenAI API	Complex documents, OCR needs

How to pick:

Need LLM-ready content? PyMuPDF4LLM is specifically designed for this
Working with multiple document formats? Markitdown handles diverse inputs
Extracting academic references? GROBID remains the standard
Building a production pipeline? Unstructured offers the best integration options
Handling complex layouts? Consider commercial OCR like Mistral or Azure Document Intelligence

The optimal approach depends on your specific requirements regarding accuracy, structure preservation, and the intended use of the extracted content.

Tips for Optimal PDF Conversion

Pre-process PDFs when possible:

# Optimize a PDF for text extraction first
ocrmypdf --optimize 3 --skip-text input.pdf optimized.pdf
Copy to clipboard
Error
Copied

Try multiple tools on the same document to compare results:

Handle scanned PDFs appropriately:

# For scanned documents, run OCR first
ocrmypdf --force-ocr input.pdf ocr_ready.pdf
PYTHONUTF8=1 uvx markitdown ocr_ready.pdf > markitdown.md
Copy to clipboard
Error
Copied

Consider post-processing for better results:

# Simple post-processing example
sed -i 's/\([A-Z]\)\./\1\.\n/g' output.md  # Add line breaks after sentences
Copy to clipboard
Error
Copied
 Previous
Scraping PDFs with Tabula
Next 
Convert HTML to Markdown


--- Convert HTML to Markdown ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
Converting HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Converting HTML to Markdown

When working with web content, converting HTML files to plain text or Markdown is a common requirement for content extraction, analysis, and preservation. For example:

Content analysis: Extract clean text from HTML for natural language processing
Data mining: Strip formatting to focus on the actual content
Offline reading: Convert web pages to readable formats for e-readers or offline consumption
Content migration: Move content between different CMS platforms
SEO analysis: Extract headings, content structure, and text for optimization
Archive creation: Store web content in more compact, preservation-friendly formats
Accessibility: Convert content to formats that work better with screen readers

This tutorial covers both converting existing HTML files and combining web crawling with HTML-to-text conversion in a single workflow – all using the command line.

defuddle-cli

defuddle-cli specializes in HTML - Markdown conversion. It’s a bit slow and not very customizable but produces clean Markdown that preserves structure, links, and basic formatting. Best for content where preserving the document structure is important.

find . -name '*.html' -exec npx --package defuddle-cli -y defuddle parse {} --md -o {}.md \;
Copy to clipboard
Error
Copied
find . -name '*.html': Finds all HTML files in the current directory and subdirectories
-exec ... \;: Executes the following command for each file found
npx --package defuddle-cli -y: Installs and runs defuddle-cli without prompting
defuddle parse {} --md: Parses the HTML file (represented by {}) and converts to markdown
-o {}.md: Outputs to a file with the original name plus .md extension
Pandoc

Pandoc is a bit slow and highly customizable, preserving almost all formatting elements, leading to verbose markdown. Best for academic or documentation conversion where precision matters.

Pandoc can convert from many other formats (such as Word, PDF, LaTeX, etc.) to Markdown and vice versa, making it one of most popular and versatele document convertors.

find . -name '*.html' -exec pandoc -f html -t markdown_strict -o {}.md {} \;
Copy to clipboard
Error
Copied
find . -name '*.html': Finds all HTML files in the current directory and subdirectories
-exec ... \;: Executes the following command for each file found
pandoc: The Swiss Army knife of document conversion
-f html -t markdown_strict: Convert from HTML format to strict markdown
-o {}.md {}: Output to a markdown file, with the input file as the last argument
Lynx

Lynx is fast and generates text (not Markdown) with minimal formatting. Lynx renders the HTML as it would appear in a text browser, preserving basic structure but losing complex formatting. Best for quick content extraction or when processing large numbers of files.

find . -type f -name '*.html' -exec sh -c 'for f; do lynx -dump -nolist "$f" > "${f%.html}.txt"; done' _ {} +
Copy to clipboard
Error
Copied
find . -type f -name '*.html': Finds all HTML files in the current directory and subdirectories
-exec sh -c '...' _ {} +: Executes a shell command with batched files for efficiency
for f; do ... done: Loops through each file in the batch
lynx -dump -nolist "$f": Uses the lynx text browser to render HTML as plain text
-dump: Output the rendered page to stdout
-nolist: Don’t include the list of links at the end
> "${f%.html}.txt": Save output to a .txt file with the same base name
w3m

w3m is very slow processing with minimal formatting. w3m tends to be more thorough in its rendering than lynx but takes considerably longer. It supports basic JavaScript processing, making it better at handling modern websites with dynamic content. Best for cases where you need slightly better rendering than lynx, particularly for complex layouts and tables, and when some JavaScript processing is beneficial.

find . -type f -name '*.html' \
  -exec sh -c 'for f; do \
      w3m -dump -T text/html -cols 80 -no-graph "$f" > "${f%.html}.md"; \
    done' _ {} +
Copy to clipboard
Error
Copied
find . -type f -name '*.html': Finds all HTML files in the current directory and subdirectories
-exec sh -c '...' _ {} +: Executes a shell command with batched files for efficiency
for f; do ... done: Loops through each file in the batch
w3m -dump -T text/html -cols 80 -no-graph "$f": Uses the w3m text browser to render HTML
-dump: Output the rendered page to stdout
-T text/html: Specify input format as HTML
-cols 80: Set output width to 80 columns
-no-graph: Don’t show graphic characters for tables and frames
> "${f%.html}.md": Save output to a .md file with the same base name
Comparison
Approach	Speed	Format Quality	Preservation	Best For
defuddle-cli	Slow	High	Good structure and links	Content migration, publishing
pandoc	Slow	Very High	Almost everything	Academic papers, documentation
lynx	Fast	Low	Basic structure only	Quick extraction, large batches
w3m	Very Slow	Medium-Low	Basic structure with better tables	Improved readability over lynx
Optimize Batch Processing

Process in parallel: Use GNU Parallel for multi-core processing:

find . -name "*.html" | parallel "pandoc -f html -t markdown_strict -o {}.md {}"
Copy to clipboard
Error
Copied

Filter files before processing:

find . -name "*.html" -type f -size -1M -exec pandoc -f html -t markdown {} -o {}.md \;
Copy to clipboard
Error
Copied

Customize output format with additional parameters:

# For pandoc, preserve line breaks but simplify other formatting
find . -name "*.html" -exec pandoc -f html -t markdown --wrap=preserve --atx-headers {} -o {}.md \;
Copy to clipboard
Error
Copied

Handle errors gracefully:

find . -name "*.html" -exec sh -c 'for f; do pandoc -f html -t markdown "$f" -o "${f%.html}.md" 2>/dev/null || echo "Failed: $f" >> conversion_errors.log; done' _ {} +
Copy to clipboard
Error
Copied
Choosing the Right Tool
Need speed with minimal formatting? Use the lynx approach
Need precise, complete conversion? Use pandoc
Need a balance of structure and cleanliness? Try defuddle-cli
Working with complex tables? w3m might render them better

Remember that the best approach depends on your specific use case, volume of files, and how you intend to use the converted text.

Combined Crawling and Conversion

Sometimes you need to both crawl a website and convert its content to markdown or text in a single workflow, like Crawl4AI or markdown-crawler.

For research/data collection: Use a specialized crawler (like Crawl4AI) with post-processing conversion
For simple website archiving: Markdown-crawler provides a convenient all-in-one solution
For high-quality conversion: Use wget/wget2 for crawling followed by pandoc for conversion
For maximum speed: Combine wget with lynx in a pipeline
Crawl4AI

Crawl4AI is designed for single-page extraction with high-quality content processing. Crawl4AI is optimized for AI training data extraction, focusing on clean, structured content rather than complete site preservation. It excels at removing boilerplate content and preserving the main article text.

uv venv
source .venv/bin/activate.fish
uv pip install crawl4ai
crawl4ai-setup
Copy to clipboard
Error
Copied
uv venv: Creates a Python virtual environment using uv (a faster alternative to virtualenv)
source .venv/bin/activate.fish: Activates the virtual environment (fish shell syntax)
uv pip install crawl4ai: Installs the crawl4ai package
crawl4ai-setup: Initializes crawl4ai’s required dependencies
markdown-crawler

markdown-crawler combines web crawling with markdown conversion in one tool. It’s efficient for bulk processing but tends to produce lower-quality markdown conversion compared to specialized converters like pandoc or defuddle. Best for projects where quantity and integration are more important than perfect formatting.

uv venv
source .venv/bin/activate.fish
uv pip install markdown-crawler
markdown-crawler -t 5 -d 3 -b ./markdown https://study.iitm.ac.in/ds/
Copy to clipboard
Error
Copied
uv venv and activation: Same as above
uv pip install markdown-crawler: Installs the markdown-crawler package
markdown-crawler: Runs the crawler with these options:
-t 5: Sets 5 threads for parallel crawling
-d 3: Limits crawl depth to 3 levels
-b ./markdown: Sets the base output directory
Final argument is the starting URL
 Previous
Convert PDFs to Markdown
Next 
LLM Website Scraping


--- Scraping: Live Sessions ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
Scraping with Excel
Scraping with Google Sheets
Crawling with the CLI
BBC Weather API with Python
Scraping IMDb with JavaScript
Nominatim API with Python
Wikipedia Data with Python
Scraping PDFs with Tabula
Convert PDFs to Markdown
Convert HTML to Markdown
LLM Website Scraping
LLM Video Screen-Scraping
Web Automation with Playwright
Scheduled Scraping with GitHub Actions
Scraping emarketer.com
Scraping: Live Sessions
Scraping: Live Sessions
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Scraping: Live Sessions

Fundamentals of web scraping with urllib and BeautifulSoup

Intermediate web scraping use of cookies

XML intro and scraping

 Previous
Scraping emarketer.com
Next 
5. Data Preparation


--- 5. Data Preparation ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation

Data preparation is crucial because raw data is rarely perfect.

It often contains errors, inconsistencies, or missing values. For example, marks data may have ‘NA’ or ‘absent’ for non-attendees, which you need to handle.

This section teaches you how to clean up data, convert it to different formats, aggregate it if required, and get a feel for the data before you analyze.

Here are links used in the video:

Presentation used in the video
Scraping assembly elections - Notebook
Assembly election results (CSV)
pdftotext software
OpenRefine software
The most persistent party
TN assembly election cartogram

 Previous
Scraping: Live Sessions
Next 
Data Cleansing in Excel


--- Data Transformation in Excel ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Transformation in Excel

You’ll learn data transformation techniques in Excel, covering:

Calculating Ratios: Compute metro area to city area and metro population to city population ratios.
Using Pivot Tables: Create pivot tables to aggregate data and identify outliers.
Filtering Data: Apply filters in pivot tables to analyze specific subsets of data.
Counting Data Occurrences: Use pivot tables to count the frequency of specific entries.
Creating Charts: Generate charts from pivot table data to visualize distributions and outliers.

Here are links used in the video:

List of Largest Cities Excel file
 Previous
Data Cleansing in Excel
Next 
Splitting Text in Excel


--- Splitting Text in Excel ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Splitting Text in Excel

You’ll learn how to transform a single-column data set into multiple, organized columns based on specific delimiters using the “Text to Columns” feature.

Here are links used in the video:

US Senate Legislation - Votes
 Previous
Data Transformation in Excel
Next 
Data Aggregation in Excel


--- Data Aggregation in Excel ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Aggregation in Excel

You’ll learn data aggregation and visualization techniques in Excel, covering:

Data Cleanup: Remove empty columns and rows with missing values.
Creating Excel Tables: Convert raw data into tables for easier manipulation and formula application.
Date Manipulation: Extract week, month, and year from date columns using Excel functions (WEEKNUM, TEXT).
Color Scales: Apply color scales to visualize clusters and trends in data over time.
Pivot Tables: Create pivot tables to aggregate data by location and date, summarizing values weekly and monthly.
Sparklines: Use sparklines to visualize trends within pivot tables, making data patterns more apparent.
Data Bars: Implement data bars for graphical illustrations of numerical columns, showing trends and waves.

Here are links used in the video:

COVID-19 data Excel file - raw data
 Previous
Splitting Text in Excel
Next 
Data Preparation in the Shell


--- Data Preparation in the Shell ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Download logs ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- List files ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Uncompress the log file ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Preview the logs ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Count requests ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Extract the IP column ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Find lines matching an IP ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Find bots ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Convert logs to CSV ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- More commands ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Shell
Download logs
List files
Uncompress the log file
Preview the logs
Count requests
Extract the IP column
Find lines matching an IP
Find bots
Convert logs to CSV
More commands
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Shell

You’ll learn how to use UNIX tools to process and clean data, covering:

curl (or wget) to fetch data from websites.
gzip (or xz) to compress and decompress files.
wc to count lines, words, and characters in text.
head and tail to get the start and end of files.
cut to extract specific columns from text.
uniq to de-duplicate lines.
sort to sort lines.
grep to filter lines containing specific text.
sed to search and replace text.
awk for more complex text processing.

Data preparation in the shell - Notebook

UNIX has a great set of tools to clean and analyze data.

This is important because these tools are:

Agile: You can quickly explore data and see the results.
Fast: They’re written in C. They’re easily parallelizable.
Popular: Most systems and languages support shell commands.

In this notebook, we’ll explore log files with these shell-based commands.

Download logs

This file has Apache web server logs for the site s-anand.net in the month of April 2024.

You can download files using wget or curl. One of these is usually available by default on most systems.

We’ll use curl to download the file from the URL https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download

# curl has LOTs of options. You won't remember most, but it's fun to geek out.
!curl --help all
Copy to clipboard
Error
Copied
Usage: curl [options...] <url>
     --abstract-unix-socket <path> Connect via abstract Unix domain socket
     --alt-svc <file name> Enable alt-svc with this cache file
     --anyauth            Pick any authentication method
 -a, --append             Append to target file when uploading
     --aws-sigv4 <provider1[:provider2[:region[:service]]]> Use AWS V4 signature authentication
     --basic              Use HTTP Basic Authentication
     --cacert <file>      CA certificate to verify peer against
     --capath <dir>       CA directory to verify peer against
 -E, --cert <certificate[:password]> Client certificate file and password
     --cert-status        Verify the status of the server cert via OCSP-staple
     --cert-type <type>   Certificate type (DER/PEM/ENG)
     --ciphers <list of ciphers> SSL ciphers to use
     --compressed         Request compressed response
     --compressed-ssh     Enable SSH compression
 -K, --config <file>      Read config from a file
     --connect-timeout <fractional seconds> Maximum time allowed for connection
     --connect-to <HOST1:PORT1:HOST2:PORT2> Connect to host
 -C, --continue-at <offset> Resumed transfer offset
 -b, --cookie <data|filename> Send cookies from string/file
 -c, --cookie-jar <filename> Write cookies to <filename> after operation
     --create-dirs        Create necessary local directory hierarchy
     --create-file-mode <mode> File mode for created files
     --crlf               Convert LF to CRLF in upload
     --crlfile <file>     Use this CRL list
     --curves <algorithm list> (EC) TLS key exchange algorithm(s) to request
 -d, --data <data>        HTTP POST data
     --data-ascii <data>  HTTP POST ASCII data
     --data-binary <data> HTTP POST binary data
     --data-raw <data>    HTTP POST data, '@' allowed
     --data-urlencode <data> HTTP POST data url encoded
     --delegation <LEVEL> GSS-API delegation permission
     --digest             Use HTTP Digest Authentication
 -q, --disable            Disable .curlrc
     --disable-eprt       Inhibit using EPRT or LPRT
     --disable-epsv       Inhibit using EPSV
     --disallow-username-in-url Disallow username in url
     --dns-interface <interface> Interface to use for DNS requests
     --dns-ipv4-addr <address> IPv4 address to use for DNS requests
     --dns-ipv6-addr <address> IPv6 address to use for DNS requests
     --dns-servers <addresses> DNS server addrs to use
     --doh-cert-status    Verify the status of the DoH server cert via OCSP-staple
     --doh-insecure       Allow insecure DoH server connections
     --doh-url <URL>      Resolve host names over DoH
 -D, --dump-header <filename> Write the received headers to <filename>
     --egd-file <file>    EGD socket path for random data
     --engine <name>      Crypto engine to use
     --etag-compare <file> Pass an ETag from a file as a custom header
     --etag-save <file>   Parse ETag from a request and save it to a file
     --expect100-timeout <seconds> How long to wait for 100-continue
 -f, --fail               Fail silently (no output at all) on HTTP errors
     --fail-early         Fail on first transfer error, do not continue
     --fail-with-body     Fail on HTTP errors but save the body
     --false-start        Enable TLS False Start
 -F, --form <name=content> Specify multipart MIME data
     --form-escape        Escape multipart form field/file names using backslash
     --form-string <name=string> Specify multipart MIME data
     --ftp-account <data> Account data string
     --ftp-alternative-to-user <command> String to replace USER [name]
     --ftp-create-dirs    Create the remote dirs if not present
     --ftp-method <method> Control CWD usage
     --ftp-pasv           Use PASV/EPSV instead of PORT
 -P, --ftp-port <address> Use PORT instead of PASV
     --ftp-pret           Send PRET before PASV
     --ftp-skip-pasv-ip   Skip the IP address for PASV
     --ftp-ssl-ccc        Send CCC after authenticating
     --ftp-ssl-ccc-mode <active/passive> Set CCC mode
     --ftp-ssl-control    Require SSL/TLS for FTP login, clear for transfer
 -G, --get                Put the post data in the URL and use GET
 -g, --globoff            Disable URL sequences and ranges using {} and []
     --happy-eyeballs-timeout-ms <milliseconds> Time for IPv6 before trying IPv4
     --haproxy-protocol   Send HAProxy PROXY protocol v1 header
 -I, --head               Show document info only
 -H, --header <header/@file> Pass custom header(s) to server
 -h, --help <category>    Get help for commands
     --hostpubmd5 <md5>   Acceptable MD5 hash of the host public key
     --hostpubsha256 <sha256> Acceptable SHA256 hash of the host public key
     --hsts <file name>   Enable HSTS with this cache file
     --http0.9            Allow HTTP 0.9 responses
 -0, --http1.0            Use HTTP 1.0
     --http1.1            Use HTTP 1.1
     --http2              Use HTTP 2
     --http2-prior-knowledge Use HTTP 2 without HTTP/1.1 Upgrade
     --http3              Use HTTP v3
     --ignore-content-length Ignore the size of the remote resource
 -i, --include            Include protocol response headers in the output
 -k, --insecure           Allow insecure server connections
     --interface <name>   Use network INTERFACE (or address)
 -4, --ipv4               Resolve names to IPv4 addresses
 -6, --ipv6               Resolve names to IPv6 addresses
 -j, --junk-session-cookies Ignore session cookies read from file
     --keepalive-time <seconds> Interval time for keepalive probes
     --key <key>          Private key file name
     --key-type <type>    Private key file type (DER/PEM/ENG)
     --krb <level>        Enable Kerberos with security <level>
     --libcurl <file>     Dump libcurl equivalent code of this command line
     --limit-rate <speed> Limit transfer speed to RATE
 -l, --list-only          List only mode
     --local-port <num/range> Force use of RANGE for local port numbers
 -L, --location           Follow redirects
     --location-trusted   Like --location, and send auth to other hosts
     --login-options <options> Server login options
     --mail-auth <address> Originator address of the original email
     --mail-from <address> Mail from this address
     --mail-rcpt <address> Mail to this address
     --mail-rcpt-allowfails Allow RCPT TO command to fail for some recipients
 -M, --manual             Display the full manual
     --max-filesize <bytes> Maximum file size to download
     --max-redirs <num>   Maximum number of redirects allowed
 -m, --max-time <fractional seconds> Maximum time allowed for transfer
     --metalink           Process given URLs as metalink XML file
     --negotiate          Use HTTP Negotiate (SPNEGO) authentication
 -n, --netrc              Must read .netrc for user name and password
     --netrc-file <filename> Specify FILE for netrc
     --netrc-optional     Use either .netrc or URL
 -:, --next               Make next URL use its separate set of options
     --no-alpn            Disable the ALPN TLS extension
 -N, --no-buffer          Disable buffering of the output stream
     --no-keepalive       Disable TCP keepalive on the connection
     --no-npn             Disable the NPN TLS extension
     --no-progress-meter  Do not show the progress meter
     --no-sessionid       Disable SSL session-ID reusing
     --noproxy <no-proxy-list> List of hosts which do not use proxy
     --ntlm               Use HTTP NTLM authentication
     --ntlm-wb            Use HTTP NTLM authentication with winbind
     --oauth2-bearer <token> OAuth 2 Bearer Token
 -o, --output <file>      Write to file instead of stdout
     --output-dir <dir>   Directory to save files in
 -Z, --parallel           Perform transfers in parallel
     --parallel-immediate Do not wait for multiplexing (with --parallel)
     --parallel-max <num> Maximum concurrency for parallel transfers
     --pass <phrase>      Pass phrase for the private key
     --path-as-is         Do not squash .. sequences in URL path
     --pinnedpubkey <hashes> FILE/HASHES Public key to verify peer against
     --post301            Do not switch to GET after following a 301
     --post302            Do not switch to GET after following a 302
     --post303            Do not switch to GET after following a 303
     --preproxy [protocol://]host[:port] Use this proxy first
 -#, --progress-bar       Display transfer progress as a bar
     --proto <protocols>  Enable/disable PROTOCOLS
     --proto-default <protocol> Use PROTOCOL for any URL missing a scheme
     --proto-redir <protocols> Enable/disable PROTOCOLS on redirect
 -x, --proxy [protocol://]host[:port] Use this proxy
     --proxy-anyauth      Pick any proxy authentication method
     --proxy-basic        Use Basic authentication on the proxy
     --proxy-cacert <file> CA certificate to verify peer against for proxy
     --proxy-capath <dir> CA directory to verify peer against for proxy
     --proxy-cert <cert[:passwd]> Set client certificate for proxy
     --proxy-cert-type <type> Client certificate type for HTTPS proxy
     --proxy-ciphers <list> SSL ciphers to use for proxy
     --proxy-crlfile <file> Set a CRL list for proxy
     --proxy-digest       Use Digest authentication on the proxy
     --proxy-header <header/@file> Pass custom header(s) to proxy
     --proxy-insecure     Do HTTPS proxy connections without verifying the proxy
     --proxy-key <key>    Private key for HTTPS proxy
     --proxy-key-type <type> Private key file type for proxy
     --proxy-negotiate    Use HTTP Negotiate (SPNEGO) authentication on the proxy
     --proxy-ntlm         Use NTLM authentication on the proxy
     --proxy-pass <phrase> Pass phrase for the private key for HTTPS proxy
     --proxy-pinnedpubkey <hashes> FILE/HASHES public key to verify proxy with
     --proxy-service-name <name> SPNEGO proxy service name
     --proxy-ssl-allow-beast Allow security flaw for interop for HTTPS proxy
     --proxy-ssl-auto-client-cert Use auto client certificate for proxy (Schannel)
     --proxy-tls13-ciphers <ciphersuite list> TLS 1.3 proxy cipher suites
     --proxy-tlsauthtype <type> TLS authentication type for HTTPS proxy
     --proxy-tlspassword <string> TLS password for HTTPS proxy
     --proxy-tlsuser <name> TLS username for HTTPS proxy
     --proxy-tlsv1        Use TLSv1 for HTTPS proxy
 -U, --proxy-user <user:password> Proxy user and password
     --proxy1.0 <host[:port]> Use HTTP/1.0 proxy on given port
 -p, --proxytunnel        Operate through an HTTP proxy tunnel (using CONNECT)
     --pubkey <key>       SSH Public key file name
 -Q, --quote <command>    Send command(s) to server before transfer
     --random-file <file> File for reading random data from
 -r, --range <range>      Retrieve only the bytes within RANGE
     --raw                Do HTTP "raw"; no transfer decoding
 -e, --referer <URL>      Referrer URL
 -J, --remote-header-name Use the header-provided filename
 -O, --remote-name        Write output to a file named as the remote file
     --remote-name-all    Use the remote file name for all URLs
 -R, --remote-time        Set the remote file's time on the local output
 -X, --request <method>   Specify request method to use
     --request-target <path> Specify the target for this request
     --resolve <[+]host:port:addr[,addr]...> Resolve the host+port to this address
     --retry <num>        Retry request if transient problems occur
     --retry-all-errors   Retry all errors (use with --retry)
     --retry-connrefused  Retry on connection refused (use with --retry)
     --retry-delay <seconds> Wait time between retries
     --retry-max-time <seconds> Retry only within this period
     --sasl-authzid <identity> Identity for SASL PLAIN authentication
     --sasl-ir            Enable initial response in SASL authentication
     --service-name <name> SPNEGO service name
 -S, --show-error         Show error even when -s is used
 -s, --silent             Silent mode
     --socks4 <host[:port]> SOCKS4 proxy on given host + port
     --socks4a <host[:port]> SOCKS4a proxy on given host + port
     --socks5 <host[:port]> SOCKS5 proxy on given host + port
     --socks5-basic       Enable username/password auth for SOCKS5 proxies
     --socks5-gssapi      Enable GSS-API auth for SOCKS5 proxies
     --socks5-gssapi-nec  Compatibility with NEC SOCKS5 server
     --socks5-gssapi-service <name> SOCKS5 proxy service name for GSS-API
     --socks5-hostname <host[:port]> SOCKS5 proxy, pass host name to proxy
 -Y, --speed-limit <speed> Stop transfers slower than this
 -y, --speed-time <seconds> Trigger 'speed-limit' abort after this time
     --ssl                Try SSL/TLS
     --ssl-allow-beast    Allow security flaw to improve interop
     --ssl-auto-client-cert Use auto client certificate (Schannel)
     --ssl-no-revoke      Disable cert revocation checks (Schannel)
     --ssl-reqd           Require SSL/TLS
     --ssl-revoke-best-effort Ignore missing/offline cert CRL dist points
 -2, --sslv2              Use SSLv2
 -3, --sslv3              Use SSLv3
     --stderr <file>      Where to redirect stderr
     --styled-output      Enable styled output for HTTP headers
     --suppress-connect-headers Suppress proxy CONNECT response headers
     --tcp-fastopen       Use TCP Fast Open
     --tcp-nodelay        Use the TCP_NODELAY option
 -t, --telnet-option <opt=val> Set telnet option
     --tftp-blksize <value> Set TFTP BLKSIZE option
     --tftp-no-options    Do not send any TFTP options
 -z, --time-cond <time>   Transfer based on a time condition
     --tls-max <VERSION>  Set maximum allowed TLS version
     --tls13-ciphers <ciphersuite list> TLS 1.3 cipher suites to use
     --tlsauthtype <type> TLS authentication type
     --tlspassword <string> TLS password
     --tlsuser <name>     TLS user name
 -1, --tlsv1              Use TLSv1.0 or greater
     --tlsv1.0            Use TLSv1.0 or greater
     --tlsv1.1            Use TLSv1.1 or greater
     --tlsv1.2            Use TLSv1.2 or greater
     --tlsv1.3            Use TLSv1.3 or greater
     --tr-encoding        Request compressed transfer encoding
     --trace <file>       Write a debug trace to FILE
     --trace-ascii <file> Like --trace, but without hex output
     --trace-time         Add time stamps to trace/verbose output
     --unix-socket <path> Connect through this Unix domain socket
 -T, --upload-file <file> Transfer local FILE to destination
     --url <url>          URL to work with
 -B, --use-ascii          Use ASCII/text transfer
 -u, --user <user:password> Server user and password
 -A, --user-agent <name>  Send User-Agent <name> to server
 -v, --verbose            Make the operation more talkative
 -V, --version            Show version number and quit
 -w, --write-out <format> Use output FORMAT after completion
     --xattr              Store metadata in extended file attributes
Copy to clipboard
Error
Copied
# We're using 3 curl options here:
#   --continue-at - continues the download from where it left off. It won't download if already downloaded
#   --location downloads the file even if the link sends us somewhere else
#   --output FILE saves the downloaded output as
!curl --continue-at - \
  --location \
  --output s-anand.net-Apr-2024.gz \
  https://drive.usercontent.google.com/uc?id=1J1ed4iHFAiS1Xq55aP858OEyEMQ-uMnE&export=download
Copy to clipboard
Error
Copied
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
100 5665k  100 5665k    0     0  3139k      0  0:00:01  0:00:01 --:--:-- 9602k
Copy to clipboard
Error
Copied
List files

ls lists files. It too has lots of options.

!ls --help
Copy to clipboard
Error
Copied
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all                  do not ignore entries starting with .
  -A, --almost-all           do not list implied . and ..
      --author               with -l, print the author of each file
  -b, --escape               print C-style escapes for nongraphic characters
      --block-size=SIZE      with -l, scale sizes by SIZE when printing them;
                               e.g., '--block-size=M'; see SIZE format below
  -B, --ignore-backups       do not list implied entries ending with ~
  -c                         with -lt: sort by, and show, ctime (time of last
                               modification of file status information);
                               with -l: show ctime and sort by name;
                               otherwise: sort by ctime, newest first
  -C                         list entries by columns
      --color[=WHEN]         colorize the output; WHEN can be 'always' (default
                               if omitted), 'auto', or 'never'; more info below
  -d, --directory            list directories themselves, not their contents
  -D, --dired                generate output designed for Emacs' dired mode
  -f                         do not sort, enable -aU, disable -ls --color
  -F, --classify             append indicator (one of */=>@|) to entries
      --file-type            likewise, except do not append '*'
      --format=WORD          across -x, commas -m, horizontal -x, long -l,
                               single-column -1, verbose -l, vertical -C
      --full-time            like -l --time-style=full-iso
  -g                         like -l, but do not list owner
      --group-directories-first
                             group directories before files;
                               can be augmented with a --sort option, but any
                               use of --sort=none (-U) disables grouping
  -G, --no-group             in a long listing, don't print group names
  -h, --human-readable       with -l and -s, print sizes like 1K 234M 2G etc.
      --si                   likewise, but use powers of 1000 not 1024
  -H, --dereference-command-line
                             follow symbolic links listed on the command line
      --dereference-command-line-symlink-to-dir
                             follow each command line symbolic link
                               that points to a directory
      --hide=PATTERN         do not list implied entries matching shell PATTERN
                               (overridden by -a or -A)
      --hyperlink[=WHEN]     hyperlink file names; WHEN can be 'always'
                               (default if omitted), 'auto', or 'never'
      --indicator-style=WORD  append indicator with style WORD to entry names:
                               none (default), slash (-p),
                               file-type (--file-type), classify (-F)
  -i, --inode                print the index number of each file
  -I, --ignore=PATTERN       do not list implied entries matching shell PATTERN
  -k, --kibibytes            default to 1024-byte blocks for disk usage;
                               used only with -s and per directory totals
  -l                         use a long listing format
  -L, --dereference          when showing file information for a symbolic
                               link, show information for the file the link
                               references rather than for the link itself
  -m                         fill width with a comma separated list of entries
  -n, --numeric-uid-gid      like -l, but list numeric user and group IDs
  -N, --literal              print entry names without quoting
  -o                         like -l, but do not list group information
  -p, --indicator-style=slash
                             append / indicator to directories
  -q, --hide-control-chars   print ? instead of nongraphic characters
      --show-control-chars   show nongraphic characters as-is (the default,
                               unless program is 'ls' and output is a terminal)
  -Q, --quote-name           enclose entry names in double quotes
      --quoting-style=WORD   use quoting style WORD for entry names:
                               literal, locale, shell, shell-always,
                               shell-escape, shell-escape-always, c, escape
                               (overrides QUOTING_STYLE environment variable)
  -r, --reverse              reverse order while sorting
  -R, --recursive            list subdirectories recursively
  -s, --size                 print the allocated size of each file, in blocks
  -S                         sort by file size, largest first
      --sort=WORD            sort by WORD instead of name: none (-U), size (-S),
                               time (-t), version (-v), extension (-X)
      --time=WORD            change the default of using modification times;
                               access time (-u): atime, access, use;
                               change time (-c): ctime, status;
                               birth time: birth, creation;
                             with -l, WORD determines which time to show;
                             with --sort=time, sort by WORD (newest first)
      --time-style=TIME_STYLE  time/date format with -l; see TIME_STYLE below
  -t                         sort by time, newest first; see --time
  -T, --tabsize=COLS         assume tab stops at each COLS instead of 8
  -u                         with -lt: sort by, and show, access time;
                               with -l: show access time and sort by name;
                               otherwise: sort by access time, newest first
  -U                         do not sort; list entries in directory order
  -v                         natural sort of (version) numbers within text
  -w, --width=COLS           set output width to COLS.  0 means no limit
  -x                         list entries by lines instead of by columns
  -X                         sort alphabetically by entry extension
  -Z, --context              print any security context of each file
  -1                         list one file per line.  Avoid '\n' with -q or -b
      --help     display this help and exit
      --version  output version information and exit

The SIZE argument is an integer and optional unit (example: 10K is 10*1024).
Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).
Binary prefixes can be used, too: KiB=K, MiB=M, and so on.

The TIME_STYLE argument can be full-iso, long-iso, iso, locale, or +FORMAT.
FORMAT is interpreted like in date(1).  If FORMAT is FORMAT1<newline>FORMAT2,
then FORMAT1 applies to non-recent files and FORMAT2 to recent files.
TIME_STYLE prefixed with 'posix-' takes effect only outside the POSIX locale.
Also the TIME_STYLE environment variable sets the default style to use.

Using color to distinguish file types is disabled both by default and
with --color=never.  With --color=auto, ls emits color codes only when
standard output is connected to a terminal.  The LS_COLORS environment
variable can change the settings.  Use the dircolors command to set it.

Exit status:
 0  if OK,
 1  if minor problems (e.g., cannot access subdirectory),
 2  if serious trouble (e.g., cannot access command-line argument).

GNU coreutils online help: <https://www.gnu.org/software/coreutils/>
Full documentation <https://www.gnu.org/software/coreutils/ls>
or available locally via: info '(coreutils) ls invocation'
Copy to clipboard
Error
Copied
# By default, it just lists all file names
!ls
Copy to clipboard
Error
Copied
sample_data  s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# If we want to see the size of the file, use `-l` for the long-listing format
!ls -l
Copy to clipboard
Error
Copied
total 5672
drwxr-xr-x 1 root root    4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 5801198 Jun  9 05:18 s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
Uncompress the log file

gzip is the most popular compression format on the web. It’s fast and pretty good. (xz is much better but slower.)

Since the file has a .gz extension, we know it’s compressed using gzip. We can use gzip -d FILE.gz to decompress the file. It’ll replace FILE.gz with FILE.

(Compression works the opposite way. gzip FILE replaces FILE with FILE.gz)link text

# gzip -d is the same as gunzip. They both decompress a GZIP-ed file
!gzip -d s-anand.net-Apr-2024.gz
Copy to clipboard
Error
Copied
# Let's list the files and see the size
!ls -l
Copy to clipboard
Error
Copied
total 50832
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

In this case, a file that was ~5.8MiB became ~52MiB, roughly 10 times larger. Clearly, it’s more efficient to store and transport compressed files – especitally if they’re plain text.

Preview the logs

To see the first few lines or the last few lines of a text file, use head or tailitalicized text

# Show the first 5 lines
!head -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
17.241.219.11 - - [31/Mar/2024:07:16:50 -0500] "GET /hindi/Hari_Puttar_-_A_Comedy_of_Terrors~Meri_Yaadon_Mein_Hai_Tu HTTP/1.1" 200 2839 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
17.241.75.154 - - [31/Mar/2024:07:17:40 -0500] "GET /hindimp3/~AAN_MILO_SAJNA%3DRANG_RANG_KE_PHOOL_KHILE HTTP/1.1" 200 2786 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
101.44.248.120 - - [31/Mar/2024:07:19:03 -0500] "GET /hindi/BRAHMCHARI HTTP/1.1" 200 2757 "http://www.s-anand.net/hindi/BRAHMCHARI" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
17.241.227.200 - - [31/Mar/2024:07:19:31 -0500] "GET /malayalam/Kaarunyam~Valampiri_Sangil HTTP/1.1" 200 2749 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.1.1 Safari/605.1.15 (Applebot/0.1; +http://www.apple.com/go/applebot)" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied
# Show the last 5 files
!tail -n 5 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
47.128.125.180 - - [30/Apr/2024:07:07:47 -0500] "GET /tamil/Subramaniyapuram HTTP/1.1" 406 226 "-" "Mozilla/5.0 (compatible; Bytespider; spider-feedback@bytedance.com) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.0.0 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:10:27 -0500] "GET /blog/bollywood-actress-jigsaw-quiz/feed/ HTTP/1.1" 200 1072 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
40.77.167.48 - - [30/Apr/2024:07:11:10 -0500] "GET /tamilmp3 HTTP/1.1" 200 4157 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
52.167.144.19 - - [30/Apr/2024:07:11:15 -0500] "GET /malayalam/Ayirathil%20Oruvan HTTP/1.1" 403 450 "-" "Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm) Chrome/116.0.1938.76 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [30/Apr/2024:07:11:31 -0500] "GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" 200 686 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Clearly, the data is from around 31 Mar 2024 a bit after 7 am EST (GMT-5) until 30 Apr 2024, a bit after 7 am EST.

Each line is an Apache log record. It has a lot of data. Some are clear. For example, taking the last row:

37.59.21.100 is the IP address that made a request. That’s from OVH - a French cloud provider. Maybe a bot.
[30/Apr/2024:07:11:31 -0500] is the time of the request
"GET /blog/2003-mumbai-bloggers-meet-photos/feed/ HTTP/1.1" is the request made to this page
200 is the HTTP reponse status code, indicating that all’s well
686 bytes was the size of the response
"Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" is the user agent. That’s Chrome 30 – a really old versio of Chrome on Linux. Very likely a bot.
Count requests

wc counts the number of lines, words, and characters in a file. The number of lines is most often used with data.

!wc s-anand.net-Apr-2024
Copy to clipboard
Error
Copied
  208539  4194545 52044491 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

So, in Apr 2024, there were ~208K requests to the site. Useful to know.

I wonder: Who is sending most of these requests?

Let’s extract the IP addresses and count them.

Extract the IP column

We’ll use cut to cut the first column. It has 2 options that we’ll use.

--delimiter is the character that splits fields. In the log file, it’s a space. (We’ll confirm this shortly.) --fields picks the field to cut. We want field 1 (IP address)

Let’s preview this:

# Preview just the IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
17.241.219.11
17.241.75.154
101.44.248.120
17.241.227.200
37.59.21.100
Copy to clipboard
Error
Copied

We used the | operator. That passes the output to the next command, head -n 5, and gives us first 5 lines. This is called piping and is the equivalent of calling a function inside another in programming languages.

We’ll use sort to sort these IP addresses. That puts the same IP addresses next to each other.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 5
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
Copy to clipboard
Error
Copied

There are no duplicates there… maybe we need to go a bit further? Let’s check the top 25 lines.

# Preview the SORTED IP addresses from the logs
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | head -n 25
Copy to clipboard
Error
Copied
100.20.65.50
100.43.111.139
101.100.145.51
101.115.156.11
101.115.205.68
101.126.25.225
101.132.248.41
101.166.40.221
101.166.6.221
101.183.40.167
101.185.221.147
101.188.225.246
101.200.218.166
101.201.66.35
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.2.187.83
101.44.160.158
101.44.160.158
101.44.160.177
101.44.160.177
Copy to clipboard
Error
Copied

OK, there are some duplicates. Good to know.

We’ll use uniq to count the unique IP addresses. It has a --count option that displays the number of unique values.

NOTE: uniq works ONLY on sorted files. You NEED to sort first.

!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | head -n 25
Copy to clipboard
Error
Copied
      1 100.20.65.50
      1 100.43.111.139
      1 101.100.145.51
      1 101.115.156.11
      1 101.115.205.68
      1 101.126.25.225
      1 101.132.248.41
      1 101.166.40.221
      1 101.166.6.221
      1 101.183.40.167
      1 101.185.221.147
      1 101.188.225.246
      1 101.200.218.166
      1 101.201.66.35
      7 101.2.187.83
      2 101.44.160.158
      2 101.44.160.177
      2 101.44.160.189
      3 101.44.160.20
      2 101.44.160.41
      1 101.44.161.208
      1 101.44.161.71
      3 101.44.161.77
      2 101.44.161.93
      2 101.44.162.166
Copy to clipboard
Error
Copied

That’s useful. 101.2.187.83 from Colombo visited 7 times.

But I’d like to know who visited the MOST. So let’s sort it further.

sort has an option --key 1n that sorts by field 1 – the count of IP addresses in this case. The n indicates that it’s a numeric sort (so 11 appears AFTER 2).

Also, we’ll use tail instead of head to get the highest entries.

# Show the top 5 IP addresses by visits
!cut --delimiter " " --fields 1 s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail -n 5
Copy to clipboard
Error
Copied
   2560 66.249.70.6
   3010 148.251.241.12
   4245 35.86.164.73
   7800 37.59.21.100
 101255 136.243.228.193
Copy to clipboard
Error
Copied

WOW! 136.243.228.193 from Dataforseo, Ukraine, sent roughly HALF of ALL the requests!

I wonder if we can figure out what User Agent they send. Is it something that identifies itself as a bot of some kind?

Find lines matching an IP

grep searches for text in files. It uses Regular Expressions which are a powerful set of wildcards.

💡 TIP: You MUST learn regular expressions. They’re very helpful.

Here, we’ll search for all lines BEGINNING with 136.243.228.193 and having a space after that. That’s "^136.243.228.193 ". The ^ at the beginning matches the start of a line.

# Preview lines that begin with 136.243.228.193
!grep "^136.243.228.193 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
136.243.228.193 - - [31/Mar/2024:11:27:43 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [31/Mar/2024:11:31:07 -0500] "GET /kannadamp3 HTTP/1.1" 200 4162 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [03/Apr/2024:17:46:42 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [06/Apr/2024:02:58:43 -0500] "GET /Statistically_improbable_phrases.html HTTP/1.1" 301 - "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
136.243.228.193 - - [08/Apr/2024:22:38:25 -0500] "GET /robots.txt HTTP/1.1" 200 195 "-" "Mozilla/5.0 (compatible; DataForSeoBot/1.0; +https://dataforseo.com/dataforseo-bot)" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

These requests have clearly identified themselves as DataForSeoBot/1.0, which is helpful. It also seems to be crawling robots.txt to check if it’s allowed to crawl the site, which is polite.

Let’s look at the second IP address: 37.59.21.100. That seems to be from OVH, a French cloud hosting provider. Is that a bot, too?

# Preview lines that begin with 37.59.21.100
!grep "^37.59.21.100 " s-anand.net-Apr-2024 | head -n 5
Copy to clipboard
Error
Copied
37.59.21.100 - - [31/Mar/2024:07:19:41 -0500] "GET /blog/matching-misspelt-tamil-movie-names/feed/ HTTP/1.1" 200 1105 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:19:53 -0500] "GET /blog/hindi-songs-online/feed/ HTTP/1.1" 200 1382 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:24:26 -0500] "GET /blog/check-your-mobile-phones-serial-number/feed/ HTTP/1.1" 200 1572 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:33:10 -0500] "GET /blog/classical-ilayaraja-2/feed/ HTTP/1.1" 200 1286 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
37.59.21.100 - - [31/Mar/2024:07:36:33 -0500] "GET /blog/correlating-subjects/feed/ HTTP/1.1" 200 2257 "-" "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36" www.s-anand.net 192.254.190.216
Copy to clipboard
Error
Copied

Looking at the user agent, Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.66 Safari/537.36, it looks like Chrome 30 – a very old version.

Personally, I believe it’s more likely to be a bot than a French human so interested in my website that they made over 250 requests every day.

Find bots

But, I’m curious. What are the user agents that DO identify themselves as bots? Let’s use grep to find all words that match bot.

grep --only-matching will show only the matches, not the entire line.

The regular expression '\S*bot\S*' (which ChatGPT generated) finds all words that have bot.

\S matches non-space characters
\S* matches 0 or more non-space characters
# Find all words with `bot` in it
!grep --only-matching '\b\w*bot\w*\b' s-anand.net-Apr-2024 | head
Copy to clipboard
Error
Copied
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Applebot
applebot
Copy to clipboard
Error
Copied
# Count frequency of all words with `bot` in it and show the top 10
!grep --only-matching '\S*bot\S*' s-anand.net-Apr-2024 | sort | uniq --count | sort --key 1n | tail
Copy to clipboard
Error
Copied
   4134 PetalBot;+https://webmaster.petalsearch.com/site/petalbot)"
   4307 /robots.txt
   5664 bingbot/2.0;
   5664 +http://www.bing.com/bingbot.htm)
   8771 +claudebot@anthropic.com)"
   8827 +http://www.google.com/bot.html)"
   8830 Googlebot/2.1;
  13798 (Applebot/0.1;
  13798 +http://www.apple.com/go/applebot)"
 101262 +https://dataforseo.com/dataforseo-bot)"
Copy to clipboard
Error
Copied

That gives me a rough sense of who’s crawling my site.

DataForSEO
Apple
Google
Anthropic
Bing
PetalBot
Convert logs to CSV

This file is almost a CSV file separated by spaces instead of commas.

The main problem is the date. Instead of [31/Mar/2024:11:27:43 -0500] it should have been "31/Mar/2024:11:27:43 -0500"

We’ll use sed (stream editor) to replace the characters. sed is like grep but lets you replace, not just search.

(Actually, sed can do a lot more. It’s a full-fledged editor. You can insert, delete, edit, etc. programmatically. In fact, sed has truly remarkable features that this paragraph is too small to contain.)

The regular expression we will use is \[\([^]]*\)\]. The way this works is:

\[: Match the opening square bracket.
\([^]]*\): Capture everything inside the square brackets (non-greedy match for any character except ]).
\]: Match the closing square bracket.

BTW, I didn’t create this. ChatGPT did.

sed "s/abc/xyz/" FILE replaces abc with xyz in the file. We can use the regular expression above for the search and "\1" for the value – it inserts captured group enclosed in double quotes.

# Replace [datetime] etc. with "datetime" and save as log.csv
!sed 's/\[\([^]]*\)\]/"\1"/' s-anand.net-Apr-2024 > log.csv
Copy to clipboard
Error
Copied
# We should now have a log.csv that's roughly the same size as the original file.
!ls -l
Copy to clipboard
Error
Copied
total 101660
-rw-r--r-- 1 root root 52044491 Jun  9 05:19 log.csv
drwxr-xr-x 1 root root     4096 Jun  6 14:21 sample_data
-rw-r--r-- 1 root root 52044491 Jun  9 05:18 s-anand.net-Apr-2024
Copy to clipboard
Error
Copied

You can download this log.csv and open it in Excel as a CSV file with space as the delimiter.

But when I did that, I faced another problem. Some of the lines had extra columns.

That’s because the “User Agent” values sometimes contain a quote. CSV files are supposed to escape quotes with "" – two double quotes. But Apache uses \" instead.

I’ll leave it as an exercise for you to fix that.

More commands

We’ve covered the commands most often used to process data before analysis.

Here are a few more that you’ll find useful.

cat concatenates multiple files. You can join multiple log files with this, for example
awk is almost a full-fledged programming interface. It’s often used for summing up values
less lets you open and read files, scrolling through it

You can read the book Data Science at the Command Line for more tools and examples.

 Previous
Data Aggregation in Excel
Next 
Data Preparation in the Editor


--- Data Preparation in the Editor ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
Data Cleansing in Excel
Data Transformation in Excel
Splitting Text in Excel
Data Aggregation in Excel
Data Preparation in the Shell
Data Preparation in the Editor
Data Preparation in the Editor
Data Preparation in DuckDB
Cleaning Data with OpenRefine
Parsing JSON
Data Transformation with dbt
Transforming Images
Extracting Audio and Transcripts
6. Data Analysis
Project 2
7. Data Visualization
Data Preparation in the Editor

You’ll learn how to use a text editor Visual Studio Code to process and clean data, covering:

Format JSON files
Find all and multiple cursors to extract specific fields
Sort lines
Delete duplicate lines
Replace text with multiple cursors

Here are the links used in the video:

City-wise product sales JSON
 Previous
Data Preparation in the Shell
Next 
Data Preparation in DuckDB


--- 7. Data Visualization ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Data visualization

 Previous
Network Analysis in Python
Next 
Visualizing Forecasts with Excel


--- Visualizing Animated Data with PowerPoint ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Visualizing Animated Data with PowerPoint

How to make a bar chart race in PowerPoint
 Previous
Visualizing Forecasts with Excel
Next 
Visualizing Animated Data with Flourish


--- Visualizing Animated Data with Flourish ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Visualizing Animated Data with Flourish

 Previous
Visualizing Animated Data with PowerPoint
Next 
Visualizing Network Data with Kumu


--- Visualizing Network Data with Kumu ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Visualizing Network Data with Kumu

Kumu
IMDB data
Jupyter Notebook

 Previous
Visualizing Animated Data with Flourish
Next 
Visualizing Charts with Excel


--- Visualizing Charts with Excel ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Visualizing Charts with Excel

 Previous
Visualizing Network Data with Kumu
Next 
Data Visualization with Seaborn


--- Data Visualization with Seaborn ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Data Visualization with Seaborn

Seaborn is a data visualization library for Python. It’s based on Matplotlib but a bit easier to use, and a bit prettier.

 Previous
Visualizing Charts with Excel
Next 
Data Visualization with ChatGPT


--- Data Visualization with ChatGPT ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
404 - Not found


--- Actor Network Visualization ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Actor Network Visualization

Find the shortest path between Govinda & Angelina Jolie using IMDb data using Python: networkx or scikit-network.

Notebook: How this video was created
The data used to visualize the network
The shortest path between actors
IMDB data
Codebase
 Previous
Data Visualization with ChatGPT
Next 
RAWgraphs


--- RAWgraphs ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
RAWgraphs

RAWgraphs
How to make Alluvial Diagram
How to make Sankey Diagram
How to make Beeswarm Plot
How to make Bump Chart
How to make Circle Packing
How to make Treemap
How to make Streamgraph
How to make Sunburst Diagram
How to make Voronoi Diagram
How to make Hexagonal Binning
 Previous
Actor Network Visualization
Next 
Data Storytelling


--- Data Storytelling ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Data Storytelling

 Previous
RAWgraphs
Next 
Narratives with LLMs


--- Interactive Notebooks: Marimo ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
Interactive Notebooks: Marimo

Marimo is a new take on notebooks that solves some headaches of Jupyter. It runs cells reactively - when you change one cell, all dependent cells update automatically, just like a spreadsheet.

Marimo’s cells can’t be run out of order. This makes Marimo more reproducible and easier to debug, but requires a mental shift from the Jupyter/Colab way of working.

It also runs Python directly in the browser and is quite interactive. Browse the gallery of examples. With a wide variety of interactive widgets, It’s growing popular as an alternative to Streamlit for building data science web apps.

Common Operations:

# Create new notebook
uvx marimo new

# Run notebook server
uvx marimo edit notebook.py

# Export to HTML
uvx marimo export notebook.py
Copy to clipboard
Error
Copied

Best Practices:

Cell Dependencies

Keep cells focused and atomic
Use clear variable names
Document data flow between cells

Interactive Elements

# Add interactive widgets
slider = mo.ui.slider(1, 100)
# Create dynamic Markdown
mo.md(f"{slider} {"🟢" * slider.value}")
Copy to clipboard
Error
Copied

Version Control

Keep notebooks are Python files
Use Git to track changes
Publish on marimo.app for collaboration

 Previous
Narratives with LLMs
Next 
HTML Slides: RevealJS


--- HTML Slides: RevealJS ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
404 - Not found


--- Markdown Presentations: Marp ---

Tools in Data Science
Tools in Data Science
1. Development Tools
2. Deployment Tools
3. Large Language Models
Project 1
4. Data Sourcing
5. Data Preparation
6. Data Analysis
Project 2
7. Data Visualization
Visualizing Forecasts with Excel
Visualizing Animated Data with PowerPoint
Visualizing Animated Data with Flourish
Visualizing Network Data with Kumu
Visualizing Charts with Excel
Data Visualization with Seaborn
Data Visualization with ChatGPT
Actor Network Visualization
RAWgraphs
Data Storytelling
Narratives with LLMs
Interactive Notebooks: Marimo
HTML Slides: RevealJS
Markdown Presentations: Marp
404 - Not found